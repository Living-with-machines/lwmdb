{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#living-with-machines-database-lmwdb","title":"Living With Machines Database: <code>lmwdb</code>","text":"<p>A package containing database access to the Living with Machines newspaper collection\u2019s metadata, designed to facilitate quicker and easier humanities research on heterogeneous and complex newspaper data.</p> <p>Background on the development of the database is available in Metadata Enrichment in the Living with Machines Project: User-focused Collaborative Database Development in a Digital Humanities Context from the Digital Humanities 2023 book of abstracts.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#install-docker","title":"Install Docker","text":"<p>It is possible to run this code without Docker, but at present we are only maintaining it via Docker Containers so we highly recommend installing Docker to run and/or test this code locally. Instructions are available for most operating systems here: https://docs.docker.com/desktop/</p>"},{"location":"#clone-the-repository","title":"Clone the repository","text":"<p>Clone the repository via either</p> <pre><code>git clone https://github.com:living-with-machines/lwmdb.git\n</code></pre> <p>or (using a <code>GitHub</code> <code>ssh key</code>)</p> <pre><code>git clone git@github.com:living-with-machines/lwmdb.git\n</code></pre> <p>followed by:</p> <pre><code>cd lwmdb\n</code></pre>"},{"location":"#local-deploy-of-documentation","title":"Local deploy of documentation","text":"<p>If you have a local install of <code>poetry</code> you can run the documentation locally without using <code>docker</code>:</p> <pre><code>poetry install\npoetry run mkdocs serve\n</code></pre>"},{"location":"#running-locally","title":"Running locally","text":"<pre><code>docker compose -f local.yml up --build\n</code></pre> <p>Note: this uses the <code>.envs/local</code> file provided in the repo. This must not be used in production, it is simply for local development and to ease demonstrating what is required for <code>.envs/production</code>, which must be generated separately for deploying via <code>production.yml</code>.</p> <p>It will take some time to download a set of <code>docker</code> images required to run locally, after which it should attempt to start the server in the <code>django</code> container. If successful, the console should print logs resembling</p> <pre><code>lwmdb_local_django    | WARNING: This is a development server. Do not use it in a production\ndeployment. Use a production WSGI server instead.\nlwmdb_local_django    |  * Running on all addresses (0.0.0.0)\nlwmdb_local_django    |  * Running on http://127.0.0.1:8000\nlwmdb_local_django    |  * Running on http://172.20.0.4:8000\nlwmdb_local_django    | Press CTRL+C to quit\nlwmdb_local_django    |  * Restarting with stat\nlwmdb_local_django    | Performing system checks...\nlwmdb_local_django    |\nlwmdb_local_django    | System check identified no issues (0 silenced).\nlwmdb_local_django    |\nlwmdb_local_django    | Django version 4.2.1, using settings 'lwmdb.settings'\nlwmdb_local_django    | Development server is running at http://0.0.0.0:8000/\nlwmdb_local_django    | Using the Werkzeug debugger (http://werkzeug.pocoo.org/)\nlwmdb_local_django    | Quit the server with CONTROL-C.\nlwmdb_local_django    |  * Debugger is active!\nlwmdb_local_django    |  * Debugger PIN: 139-826-693\n</code></pre> <p>Indicating it\u2019s up and running. You should then be able to go to <code>http://127.0.0.1:8000</code> in your local browser and see a start page.</p> <p>To stop the app call the <code>down</code> command:</p> <pre><code>docker compose -f local.yml down\n</code></pre>"},{"location":"#importing-data","title":"Importing data","text":"<p>If a previous version of the database is available as either <code>json</code> fixtures or raw <code>sql</code> via a <code>pg_dump</code> (or similar) command.</p>"},{"location":"#json-import","title":"<code>json</code> import","text":"<p><code>json</code> <code>fixtures</code> need to be placed in a <code>fixtures</code> folder in your local checkout:</p> <pre><code>cd lwmdb\nmkdir fixtures\ncp DataProvider-1.json  Ingest-1.json Item-1.json Newspaper-1.json Digitisation-1.json Issue-1.json Item-2.json fixtures/\n</code></pre> <p>The files can then be imported via</p> <pre><code>docker compose -f local.yml exec django /app/manage.py loaddata fixtures/Newspaper-1.json\ndocker compose -f local.yml exec django /app/manage.py loaddata fixtures/Issue-1.json\ndocker compose -f local.yml exec django /app/manage.py loaddata fixtures/Item-2.json\n...\n</code></pre> <p>:warning: Note the import order is important, specifically: <code>Newspaper</code>, <code>Issue</code> and any other data <code>json</code> files prior to <code>Item</code> <code>json</code>.</p>"},{"location":"#importing-a-postgres-database","title":"Importing a <code>postgres</code> database","text":"<p>Importing from <code>json</code> can be very slow. If provided a <code>postgres</code> data file, it is possible to import that directly. First copy the database file(s) to a <code>backups</code> folder on the <code>postgres</code> instance (assuming you\u2019ve run the <code>build</code> command)</p> <pre><code>docker cp backups $(docker compose -f local.yml ps -q postgres):/backups\n</code></pre> <p>Next make sure the app is shut down, then start up with only the <code>postgres</code> container running:</p> <pre><code>docker compose -f local.yml down\ndocker compose -f local.yml up postgres\n</code></pre> <p>Then run the <code>restore</code> command with the filename of the backup. By default backup filenames indicates when the backup was made and are compressed (using <code>gzip</code> compression in the example below <code>backup_2023_04_03T07_22_10.sql.gz</code> ):</p> <p>:warning: There is a chance the default <code>docker</code> size allocated is not big enough for a full version of the dataset (especially if running on a desktop). If so, you may need to increase the allocated disk space. For example, see <code>Docker Mac FAQs</code> for instructions to increase available disk space.</p> <pre><code>docker compose -f local.yml exec postgres restore backup_2023_04_03T07_22_10.sql.gz\n</code></pre> <p>:warning: If the version of the database you are loading is not compatible with the current version of the python package, this can cause significant errors.</p>"},{"location":"#querying-the-database","title":"Querying the database","text":""},{"location":"#jupyter-notebook","title":"Jupyter Notebook","text":"<p>In order to run the Django framework inside a notebook, open another terminal window once you have it running via <code>docker</code> as described above and run</p> <pre><code>docker compose -f local.yml exec django /app/manage.py shell_plus --notebook\n</code></pre> <p>This should launch a normal Jupyter Notebook in your browser window where you can create any notebooks and access the database in different ways.</p> <p>Important: Before importing any models and working with the database data, you will want to run the <code>import django_initialiser</code> in a cell, which will set up all the dependencies needed.</p> <p>Note: For some users we provide two <code>jupyter</code> <code>notebooks</code>:</p> <ul> <li><code>getting-started.ipynb</code></li> <li><code>explore-newspapers.ipynb</code></li> </ul> <p>Both will give some overview of how one can access the database\u2019s information and what one can do with it. They only scratch the surface of what is possible, of course, but will be a good entry point for someone who wants to orient themselves toward the database and Django database querying.</p>"},{"location":"#upgrade-development-version","title":"Upgrade development version","text":"<p>In order to upgrade the current development version that you have, make sure that you have synchronised the repository to your local drive:</p> <p>Step 1: <code>git pull</code></p> <p>Step 2: <code>docker compose -f local.yml up --build</code></p>"},{"location":"#run-on-a-server","title":"Run on a server","text":"<p>To run in production, an <code>.envs/production</code> <code>ENV</code> file must be created. This must befilled in with new passwords for each key rather than a copy of <code>.envs/local</code>. The same keys set in <code>.envs/local</code> are needed, as well as the follwing two:</p> <ul> <li><code>TRAEFIK_EMAIL=\"email.register.for.traefik.account@test.com\"</code></li> <li><code>HOST_URL=\"host.for.lwmdb.deploy.org\"</code></li> </ul> <p>A domain name (in this example <code>\"host.for.lwmdb.deploy.org</code>) must be registered for <code>https</code> (encripyted) usage, and a <code>TLS</code> certificate is needed. See <code>traefik</code> docs for details.</p>"},{"location":"#contributors","title":"Contributors","text":"<sub>Kalle Westerling</sub>\ud83d\udcbb \ud83e\udd14 \ud83d\udcd6 <sub>griff-rees</sub>\ud83d\udcbb \ud83e\udd14 \ud83e\uddd1\u200d\ud83c\udfeb \ud83d\udea7 \ud83d\udcd6 <sub>Aoife Hughes</sub>\ud83d\udcbb <sub>Tim Hobson</sub>\ud83d\udcbb \ud83e\udd14 <sub>Nilo Pedrazzini</sub>\ud83d\udcbb \ud83e\udd14 <sub>Christina Last</sub>\ud83d\udcbb \ud83e\udd14 <sub>claireaustin01</sub>\ud83e\udd14 <sub>Mia</sub>\ud83d\udcbb \ud83e\udd14 \ud83d\udcd6 <sub>Andy Smith</sub>\ud83d\udcbb \ud83e\udd14 <sub>Katie McDonough</sub>\ud83d\udcbb \ud83e\udd14 <sub>Mariona</sub>\ud83d\udcbb \ud83e\udd14 <sub>Kaspar Beelen</sub>\ud83d\udcbb \ud83e\udd14 <sub>David Beavan</sub>\ud83d\udcbb \ud83e\udd14"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at https://livingwithmachines.ac.uk/contact-us/. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla\u2019s code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Please see our Code of Conduct for policies on contributing. We also broadly follow the Turing Way Code of Conduct to encourage a pleasant experience contributing and collaborating on this project.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>If you would only like to contribute to documentation, the easiest way to deploy and see changes rendered with each edit is to run outside docker:</p> <pre><code>$ git clone https://github.com/living-with-machines/lwmdb\n$ cd lwmdb\n$ poetry install --with dev --with docs\n$ poetry run mkdocs serve --dev-addr=0.0.0.0:8080\n</code></pre> <p>Note</p> <p>The <code>--with dev</code> and <code>--with docs</code> options are currently included by default, but they may be set as optional in the future.</p> <p>Documentation should also be available on <code>https://localhost:9000</code> when running </p> <pre><code>docker compose -f local.yml up\n</code></pre> <p>but it does not auto update as local changes are made. Port <code>8080</code> is specified in the example above to avoid conflict with a local <code>docker compose</code> run (which defaults to <code>0.0.0.0:9000</code>).</p> <p>Warning</p> <p>The <code>schema</code> currently raises an error. See ticket <code>#115</code> for updates.</p>"},{"location":"contributing/#local-docker-test-runs","title":"Local <code>docker</code> test runs","text":""},{"location":"contributing/#local-environment","title":"Local environment","text":"<p>Tests are built and run via <code>pytest</code> and <code>docker</code> using <code>pytest-django</code>. To run tests ensure a local <code>docker</code> install, a local <code>git</code> checkout of <code>lwmdb</code> and a build (see install instructions for details).</p> <p>Running locally with <code>local.yml</code> in a terminal deploys the site and this documentation:</p> usersudo <pre><code>docker compose -f local.yml up\n</code></pre> <pre><code>sudo docker compose -f local.yml up\n</code></pre> <ul> <li>Site at <code>localhost:3000</code></li> <li>Docs at <code>localhost:9000</code></li> </ul> <p>Note</p> <p>If there are issues starting the server, shutting it down and then starting up again may help </p> usersudo <pre><code>docker compose -f local.yml down\n</code></pre> <pre><code>sudo docker compose -f local.yml down\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running tests","text":"<p>To run tests, open another terminal to run <code>pytest</code> within the <code>django</code> <code>docker</code> <code>container</code> while <code>docker</code> is running. </p> usersudo <pre><code>docker compose -f local.yml exec django pytest\n</code></pre> <pre><code>sudo docker compose -f local.yml exec django pytest\n</code></pre> <p>These will print out a summary of test results like:</p> <pre><code>Test session starts (platform: linux, Python 3.11.3, pytest 7.3.1, pytest-sugar 0.9.7)\ndjango: settings: config.test_settings (from ini)\nrootdir: /app\nconfigfile: pyproject.toml\nplugins: pyfakefs-5.2.2, anyio-3.6.2, sugar-0.9.7, cov-4.0.0, django-4.5.2\ncollected 33 items / 1 deselected / 32 selected\n\n gazetteer/tests.py \u2713                                          3% \u258d\n lwmdb/tests/test_commands.py xx                               9% \u2589\n mitchells/tests.py x\u2713                                       100% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n newspapers/tests.py \u2713\u2713\u2713\u2713\u2713                                    28% \u2588\u2588\u258a\n lwmdb/utils.py \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713                                     56% \u2588\u2588\u2588\u2588\u2588\u258b\n lwmdb/tests/test_utils.py \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713                      97% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a\n------------ coverage: platform linux, python 3.11.3-final-0 ---------------\nName                                                     Stmts   Miss  Cover\n----------------------------------------------------------------------------\nlwmdb/management/commands/connect.py                        10      3    70%\nlwmdb/management/commands/createfixtures.py                 42     30    29%\nlwmdb/management/commands/fixtures.py                      126     78    38%\nlwmdb/management/commands/load_json_fixtures.py             20     11    45%\nlwmdb/management/commands/loadfixtures.py                   27      8    70%\nlwmdb/management/commands/makeitemfixtures.py               78     62    21%\nlwmdb/tests/test_commands.py                                15      2    87%\nlwmdb/tests/test_utils.py                                   25      7    72%\nlwmdb/utils.py                                             120     48    60%\n----------------------------------------------------------------------------\nTOTAL                                                      508    284    44%\n\n8 files skipped due to complete coverage.\n\n============================ slowest 3 durations ===========================\n3.85s setup    gazetteer/tests.py::TestGeoSpatial::test_create_place_and_distance\n1.06s call     lwmdb/tests/test_commands.py::test_mitchells\n0.14s call     lwmdb/utils.py::lwmdb.utils.download_file\n\nResults (6.74s):\n      29 passed\n       3 xfailed\n       1 deselected\n</code></pre>"},{"location":"contributing/#adding-all-expected-failed-tests","title":"Adding all expected failed tests","text":"<p>In the previous example, 29 tests passed, 3 failed as expected (hence <code>xfailed</code>) and 1 test was skipped (<code>deselected</code>). To see the deatils of what tests failed, adding the <code>--runxfail</code> option will add reports like the following:  </p> usersudo <pre><code>docker compose -f local.yml exec django pytest --runxfail\n</code></pre> <pre><code>sudo docker compose -f local.yml exec django pytest --runxfail\n</code></pre> <pre><code>...\n    def __getattr__(self, name: str):\n        \"\"\"\n        After regular attribute access, try looking up the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # Note: obj.x will always call obj.__getattribute__('x') prior to\n        # calling obj.__getattr__('x').\n        if (\n            name not in self._internal_names_set\n            and name not in self._metadata\n            and name not in self._accessors\n            and self._info_axis._can_hold_identifiers_and_holds_name(name)\n        ):\n            return self[name]\n&gt;       return object.__getattribute__(self, name)\nE       AttributeError: 'Series' object has no attribute 'NLP'\n\n/usr/local/lib/python3.11/site-packages/pandas/core/generic.py:5989: AttributeError\n-------------------------- Captured stdout call ----------------------------\nWarning: Model mitchells.Issue is missing a fixture file and will not load.\nWarning: Model mitchells.Entry is missing a fixture file and will not load.\nWarning: Model mitchells.PoliticalLeaning is missing a fixture file and will not load.\nWarning: Model mitchells.Price is missing a fixture file and will not load.\nWarning: Model mitchells.EntryPoliticalLeanings is missing a fixture file and will not load.\nWarning: Model mitchells.EntryPrices is missing a fixture file and will not load.\n\n lwmdb/tests/test_commands.py \u2a2f                           6% \u258b\n...\n</code></pre> <p>and summaries at the end of the report</p> <pre><code>...\n============================ slowest 3 durations ===========================\n3.87s setup    gazetteer/tests.py::TestGeoSpatial::test_create_place_and_distance\n1.07s call     lwmdb/tests/test_commands.py::test_mitchells\n0.15s call     lwmdb/utils.py::lwmdb.utils.download_file\n========================== short test summary info =========================\nFAILED lwmdb/tests/test_commands.py::test_mitchells - AttributeError: 'Series' object\nhas no attribute 'NLP'\nFAILED lwmdb/tests/test_commands.py::test_gazzetteer - SystemExit: App(s) not allowed: ['gazzetteer']\nFAILED mitchells/tests.py::MitchelsFixture::test_load_fixtures - assert 0 &gt; 0\n\nResults (6.90s):\n      29 passed\n       3 failed\n         - lwmdb/tests/test_commands.py:9 test_mitchells\n         - lwmdb/tests/test_commands.py:19 test_gazzetteer\n         - mitchells/tests.py:18 MitchelsFixture.test_load_fixtures\n       1 deselected\n</code></pre>"},{"location":"contributing/#terminal-interaction","title":"Terminal Interaction","text":"<p>Adding the <code>--pdb</code> option generates an <code>ipython</code> shell at the point a test fails:</p> usersudo <pre><code>docker compose -f local.yml exec django pytest --runxfail --pdb\n</code></pre> <pre><code>sudo docker compose -f local.yml exec django pytest --runxfail --pdb\n</code></pre> <pre><code>    def __getattr__(self, name: str):\n        \"\"\"\n        After regular attribute access, try looking up the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # Note: obj.x will always call obj.__getattribute__('x') prior to\n        # calling obj.__getattr__('x').\n        if (\n            name not in self._internal_names_set\n            and name not in self._metadata\n            and name not in self._accessors\n            and self._info_axis._can_hold_identifiers_and_holds_name(name)\n        ):\n            return self[name]\n&gt;       return object.__getattribute__(self, name)\nE       AttributeError: 'Series' object has no attribute 'NLP'\n\n/usr/local/lib/python3.11/site-packages/pandas/core/generic.py:5989: AttributeError\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; entering PDB &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; PDB post_mortem (IO-capturing turned off) &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt; /usr/local/lib/python3.11/site-packages/pandas/core/generic.py(5989)__getattr__()\n   5987         ):\n   5988             return self[name]\n-&gt; 5989         return object.__getattribute__(self, name)\n   5990\n   5991     def __setattr__(self, name: str, value) -&gt; None:\n\nipdb&gt;\n</code></pre>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#commits","title":"Commits","text":""},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>The <code>.pre-commit-config.yaml</code> file manages configurations to ensure quality of each <code>git</code> commit. Ensure this works by installing <code>pre-commit</code> before making any <code>git</code> commits.</p> <p>Note</p> <p><code>pre-commit</code> is included in the <code>pyproject.toml</code> <code>dev</code> dependencies group, so it\u2019s possible to run all <code>git</code> commands within a local <code>poetry</code> install of <code>lwmdb</code> without installing <code>pre-commit</code> globally.   </p> <p>This will automatically download and install dependencies specified in <code>.pre-commit-config.yaml</code> and then run all those checks for any <code>git</code> commit.</p> <p>You can run all of these checks outside a commit with</p> shellpoetry <pre><code>pre-commit run --all-files\n</code></pre> <pre><code>poetry run pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit messages","text":"<p>For git commit messages we try to follow the <code>conventional commits</code> spec, where commits are prefixed by categories:</p> <ul> <li><code>fix</code>: something fixed</li> <li><code>feat</code>: a new feature</li> <li><code>doc</code>: documentation</li> <li><code>refactor</code>: a significant rearangement code structure</li> <li><code>test</code>: adding tests</li> <li><code>ci</code>: continuous integrations</li> <li><code>chore</code>: something relatively small like updating a dependency</li> </ul>"},{"location":"contributing/#app","title":"App","text":"<p>Once <code>docker compose</code> is up, any local modifications should automatically be loaded in the local <code>django</code> <code>docker</code> <code>container</code> and immediately applied. This suits reloading web app changes (including <code>css</code> etc.) and writing and running tests. No additional <code>docker build</code> commands should be required unless very significant modifcations, such as shifting between <code>git</code> <code>branches</code>.</p>"},{"location":"contributing/#tests","title":"Tests","text":""},{"location":"contributing/#doctests","title":"Doctests","text":"<p>Including <code>docstrings</code> with example tests is an efficient way to add tests, document usage and help ensure documentation is consistent with code changes.</p>"},{"location":"contributing/#pytest-tests","title":"Pytest Tests","text":"<p>We use <code>pytest</code> for tests, and their documentation is quite comprehensive. The <code>django-pytest</code> module is crucial to the test functionality as well.</p>"},{"location":"contributing/#pytest-configuration","title":"Pytest Configuration","text":"<p>The config for running tests is shared between <code>pyproject.toml</code> and <code>lwmdb/tests/conftest.py</code>.</p> <p>The <code>pyproject.toml</code> section below provides automatic test configuration whenever <code>pytest</code> is run. An example config at the time of this writing:</p> <pre><code>[tool.pytest.ini_options]\nDJANGO_SETTINGS_MODULE = \"config.test_settings\"\npython_files = [\"tests.py\", \"test_*.py\"]\naddopts = \"\"\"\n--cov=lwmdb\n--cov-report=term:skip-covered\n--pdbcls=IPython.terminal.debugger:TerminalPdb\n--doctest-modules\n--ignore=compose\n--ignore=jupyterhub_config.py\n--ignore=notebooks\n--ignore=docs\n--ignore=lwmdb/contrib/sites\n-m \"not slow\"\n--durations=3\n\"\"\"\nmarkers = [\n  \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\"\n]\n</code></pre> <ul> <li><code>--cov=lwmdb</code> specifies the path to test (in this case the name of this project)</li> <li><code>--cov-report=term:skip-covered</code> excludes files with full coverage from the coverage report</li> <li><code>--pydbcls=Ipython.terminal.debugger:TerminalPdb</code> enables the <code>ipython</code> terminal for debugging </li> <li><code>--doctest-modules</code> indicates <code>doctests</code> are included in test running</li> <li><code>--ignore</code> excludes folders from testing (eg: <code>--ignore=compose</code> skips the <code>compose</code> folder)</li> <li><code>-m \"not slow\"</code> skips tests marked with <code>@pytest.mark.slow</code></li> <li><code>--duration=3</code> lists the duration of the 3 slowest running tests</li> </ul>"},{"location":"contributing/#example-tests","title":"Example Tests","text":"<p>Within each <code>django</code> <code>app</code> in the project there is either a <code>tests.py</code> file or a <code>tests</code> folder, where any file name beginning with <code>test_</code> is included (like <code>test_commands.py</code>).</p> <p>An example test from <code>mitchells/tests.py</code>:</p> <pre><code>def test_download_local_mitchells_excel(caplog, mitchells_data_path) -&gt; None:\n    \"\"\"Test downloading `MITCHELLS_EXCEL_URL` fixture.\n\n    Note:\n        `assert LOG in caplog.messages` is designed to work whether the file is\n        downloaded or not to ease caching and testing\n    \"\"\"\n    caplog.set_level(INFO)\n    success: bool = download_file(mitchells_data_path, MITCHELLS_EXCEL_URL)\n    assert success\n    LOG = f\"{MITCHELLS_EXCEL_URL} file available from {mitchells_data_path}\"\n    assert LOG in caplog.messages\n</code></pre> <p><code>mitchells_data_path</code> <code>fixture</code> is defined in <code>conftest.py</code> and returns a <code>Path</code> for the folder where raw <code>mitchells</code> data is stored prior to processing into <code>json</code>.</p> <p><code>Fixtures</code> in <code>pytest</code> work by automatically populating any functions names beginning with <code>test_</code> with whatever is returned from registered <code>fixture</code> functions. Here the <code>mitchells_data_path</code> <code>Path</code> object is passed to the <code>download_file</code> function and saved to <code>MITCHELLS_LOCAL_LINK_EXCEL_URL</code>. <code>download_file</code> returns a <code>bool</code> to indicate if the dowload was successful, hence then testing if the value returned is <code>True</code> via the line:</p> <pre><code>assert success\n</code></pre> <p>The lines involving <code>caplog</code> aid testing <code>logging</code>. The logging level is to <code>INFO</code> to capture levels lower than the default <code>WARNING</code> level.</p> <pre><code>caplog.set_level(INFO)\n</code></pre> <p>This then means the logging is captured and can be tested on the final line </p> <pre><code>assert caplog.messages == [\n    f'{MITCHELLS_LOCAL_LINK_EXCEL_PATH} file available from {mitchells_data_path}'\n]\n</code></pre> <p>Note</p> <p>To ease using <code>python</code> logging and <code>django</code> logging features we use our <code>log_and_django_terminal</code> wrapper to ease managing logs that might also need to be printed at the terminal alongside commands.</p>"},{"location":"contributing/#crediting-contributions","title":"Crediting Contributions","text":"<p>We use All Contributors in our semi-automated file citation file <code>.all-contributorsrc</code> and Citation File Format via <code>CITATION.cff</code> to help manage attributing contributions to both this code base and datasets we release for use with <code>lwmdb</code>. We endeavour to harmonise contributions from collaborators across Living with Machines whose copious, interdisciplinary collaboration led to <code>lwmdb</code>.</p>"},{"location":"contributing/#all-contributors","title":"All Contributors","text":"<p>All Contributors is a service for managing credit for contributions to a <code>git</code> repository. <code>.all-contributorsrc</code> is a <code>json</code> file in the root directory of the <code>alnm</code> repository. It also specifies design for what\u2019s rendered in <code>README.md</code> and intro contributors section of this documentation. </p> <p>The <code>json</code> structure follows the All Contributors <code>specification</code>. Below is an example of this format</p> <pre><code>{\n  \"files\": [\n    \"README.md\"\n  ],\n  \"imageSize\": 100,\n  \"commit\": false,\n  \"commitType\": \"docs\",\n  \"commitConvention\": \"angular\",\n  \"contributors\": [\n    {\n      \"login\": \"github-user-name\",\n      \"name\": \"Person Name\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/1234567?v=4\",\n      \"profile\": \"http://www.a-website.org\",\n      \"contributions\": [\n        \"code\",\n        \"ideas\",\n        \"doc\"\n      ]\n    },\n    {\n      \"login\": \"another-github-user-name\",\n      \"name\": \"Another Name\",\n      \"avatar_url\": \"https://avatars.githubusercontent.com/u/7654321?v=4\",\n      \"contributions\": [\n        \"code\",\n        \"ideas\",\n        \"doc\",\n        \"maintenance\"\n      ]\n    },\n  ],\n  \"contributorsPerLine\": 7,\n  \"skipCi\": true,\n  \"repoType\": \"github\",\n  \"repoHost\": \"https://github.com\",\n  \"projectName\": \"lwmdb\",\n  \"projectOwner\": \"Living-with-machines\"\n}\n</code></pre> <p>The <code>contribution</code> component per user indicates type of contributionat present we consider these: </p> <ul> <li><code>code</code></li> <li><code>ideas</code></li> <li><code>mentoring</code></li> <li><code>maintenance</code></li> <li><code>doc</code></li> </ul> <p>At present we aren\u2019t crediting other types of contribution but may expand in the future. For more other contribtuion types provided by <code>allcontributors</code> by default, see the <code>emoji-key</code> table. </p>"},{"location":"contributing/#adding-credit-including-types-via-github-comments","title":"Adding credit, including types, via GitHub comments","text":"<p>For All Contributors <code>git</code> accounts with at least <code>moderator</code> status with our <code>GitHub</code> repository should have permission to modify credit by posting in the following form on an <code>lwmdb</code> <code>github</code> ticket:</p> <pre><code>@all-contributors\nplease add @github-user for code, ideas, planning.\nplease add @github-other-user for code, ideas, planning.\n</code></pre> <p>This should cause the <code>all-contributors bot</code> to indicated success:</p> <pre><code>@ModUserWhoPosted\n\nI've put up a pull request to add @github-user! \ud83c\udf89\nI've put up a pull request to add @github-other-user! \ud83c\udf89\n</code></pre> <p>or report errors:</p> <pre><code>This project's configuration file has malformed JSON: .all-contributorsrc. Error:: Unexpected token : in JSON at position 2060\n</code></pre>"},{"location":"contributing/#citationcff","title":"CITATION.CFF","text":"<p>We also maintain a <code>Citation File Format (CFF)</code> file for citeable, academic credit for contributions via our <code>zenodo</code> registration. This helps automate the process of releasing academically citeable Digital Object Identifyer (DOI) for releases of <code>lwmdb</code>.</p> <p><code>CFF</code> supports Open Researcher and Contributor IDs (<code>orcid</code>), which eases automating academic credit for evolving contribtuions to academic work, even as individuals change academic positions.  For reference a simplified example based on <code>cff-version 1.2.0</code>:</p> <pre><code>cff-version: 1.2.0\ntitle: Living With Machines Database\nmessage: &gt;-\n  If you use this software, please cite it using the\n  metadata from this file.\ntype: software\nauthors:\n  - given-names: Person\n    family-names: Name\n    orcid: 'https://orcid.org/0000-0000-0000-0000'\n    affiliation: A UNI\n  - given-names: Another\n    family-names: Name\n    orcid: 'https://orcid.org/0000-0000-0000-0001'\n    affiliation: UNI A\nidentifiers:\n  - type: doi\n    value: 10.5281/zenodo.8208204\nrepository-code: 'https://github.com/Living-with-machines/lwmdb'\nurl: 'https://livingwithmachines.ac.uk/'\nlicense: MIT\n</code></pre>"},{"location":"contributing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/#unexpected-lwmdbstaticcssprojectcss-changes","title":"Unexpected <code>lwmdb/static/css/project.css</code> changes","text":"<p>At present (see issue #110 for updates) running <code>docker compose</code> is likely to truncate the last line of <code>/lwmdb/static/css/project.css</code> which, can then appear as a local change in a <code>git</code> checkout:</p> <pre><code>$ git status\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   lwmdb/static/css/project.css\n</code></pre> <p>This should be automatically fixed via <code>pre-commit</code>, and if necessary you can run <code>pre-commit</code> directly to clean that issue outside of a <code>git</code> commit. Given how frequently this may occur, it is safest to simply leave that until commiting a change.</p> shellpoetry <pre><code>pre-commit run --all-files\n</code></pre> <pre><code>poetry run pre-commit run --all-files\n</code></pre>"},{"location":"deploy/","title":"Deploy","text":"<p>There are two main options for deploying this data. Both require <code>docker</code> to manage build, testing and interoperability, and at the time of this writing at least 30 GB of free harddrive space, preferable two or three times that for flexibility.</p>"},{"location":"deploy/#local-deploy","title":"Local Deploy:","text":"<p>Local deploys are well suited for</p> <ul> <li>Individual research</li> <li>Testing new features</li> <li>Contributing to bug fixes or documentation</li> </ul> <p>Assuming a personal computer, <code>docker</code> Desktop is one of the easier options and works for <code>Linux</code> distributions, <code>Windows</code> and <code>macOS</code>.</p>"},{"location":"deploy/#clone-repository-to-local-drive","title":"Clone repository to local drive","text":"<p>Run the following command on your command line:</p> <pre><code>git clone git@github.com:Living-with-machines/lib_metadata_db.git\ncd lib_metadata_db\n</code></pre> <p>The subsequent sections assume commands are run from within the <code>lib_metadata_db</code> folder.</p>"},{"location":"deploy/#local-build","title":"Local Build","text":"usersudo <pre><code>docker compose -f local.yml up --build\n</code></pre> <pre><code>sudo docker compose -f local.yml up --build\n</code></pre> <p>This uses the <code>.envs/local</code> file provided in the repo. This must not be used in production, it is simply for local development and to ease demonstrating what is required for <code>.envs/production</code>, which must be generated separately for deploying via <code>production.yml</code>.</p> <p>It will take some time to download a set of <code>docker</code> images required to run locally, after which it should attempt to start the server in the <code>django</code> container. If successful, the console should print logs resembling</p> <pre><code>metadata_local_django    | WARNING: This is a development server. Do not use it in a production\ndeployment. Use a production WSGI server instead.\nmetadata_local_django    |  * Running on all addresses (0.0.0.0)\nmetadata_local_django    |  * Running on http://127.0.0.1:8000\nmetadata_local_django    |  * Running on http://172.20.0.4:8000\nmetadata_local_django    | Press CTRL+C to quit\nmetadata_local_django    |  * Restarting with stat\nmetadata_local_django    | Performing system checks...\nmetadata_local_django    |\nmetadata_local_django    | System check identified no issues (0 silenced).\nmetadata_local_django    |\nmetadata_local_django    | Django version 4.2, using settings 'metadata.settings'\nmetadata_local_django    | Development server is running at http://0.0.0.0:8000/\nmetadata_local_django    | Using the Werkzeug debugger (http://werkzeug.pocoo.org/)\nmetadata_local_django    | Quit the server with CONTROL-C.\nmetadata_local_django    |  * Debugger is active!\nmetadata_local_django    |  * Debugger PIN: 139-826-693\n</code></pre> <p>Indicating it\u2019s up and running. You should then be able to go to <code>http://127.0.0.1:8000</code> in your local browser and see a start page.</p> <p>To stop the app call the <code>down</code> command:</p> usersudo <pre><code>docker compose -f local.yml down\n</code></pre> <pre><code>sudo docker compose -f local.yml down\n</code></pre>"},{"location":"deploy/#production-deploy","title":"Production Deploy","text":"<p>A local production deploy should be available without aditional modification. Deploying for exteral users is a more involved process and will require registering a domain name.</p> <p>To run in production, a <code>.envs/production</code> <code>ENV</code> file is required. This is not provided to help ensure encryption keys are generated uniquely by users. These are specifically for </p> <ul> <li><code>SECRET_KEY</code></li> <li><code>POSTGRES_PASSWORD</code></li> </ul> <p>as well as the follwing two:</p> <ul> <li><code>TRAEFIK_EMAIL=\"email.register.for.traefik.account@test.com\"</code></li> <li><code>HOST_URL=\"host.for.lwmdb.deploy.org\"</code></li> </ul>"},{"location":"deploy/#traefik-config","title":"<code>traefik</code> Config","text":"<p>A domain name (in this example <code>\"host.for.lwmdb.deploy.org</code>) must be registered for <code>https</code> (encripyted) usage, and a <code>TLS</code> certificate is needed. See <code>traefik</code> docs for details.</p>"},{"location":"deploy/#generating-password-config","title":"Generating password config","text":"<p>There are numerous methods for generating keys. <code>python</code> provides an option via the <code>secrets</code> module:</p> <pre><code>import secrets\n\nprint(secrets.token_urlsafe())\"\n</code></pre> <p>For convenience and minimising risks like screencapture, this can be run and piped to your local clipboard. Examples for different operating systems:</p> linuxmacOSwindows <pre><code>python -c \"import secrets; print(secrets.token_urlsafe())\" | xclip\n</code></pre> <pre><code>python -c \"import secrets; print(secrets.token_urlsafe())\" | pbcopy\n</code></pre> <pre><code>python -c \"import secrets; print(secrets.token_urlsafe())\" | /dev/clipboard\n</code></pre> <p>If arranging this via a deploy service like <code>azure</code>, it is also possible to add keys/config within the local environment or via an <code>export</code> command (assuming a <code>bash</code> or <code>zsh</code> shell).</p>"},{"location":"fulltext/","title":"Accessing full-text using <code>extract_fulltext</code> method","text":"<p>We are developing a fulltext table for all articles across our available newspapers. Meanwhile, @thobson88 has developed an <code>.extract_fulltext()</code> method that can be used on any <code>Item</code> objects. Here is an example:</p> <pre><code>from newspapers.models import Newspaper\nfrom newspapers.models import Item\nfrom pathlib import Path\n\n# Set the local download path:\nItem.DOWNLOAD_DIR = Path.home() / \"temp/fulltext\"\n\n# Set the SAS token:\n%env FULLTEXT_SAS_TOKEN=\"?an=SSH&amp;token=true\"\n\nitem = Newspaper.objects.get(publication_code=\"0003040\").issues.first().items.first()\nitem.extract_fulltext()\n</code></pre> <p>If you need help setting up a SAS token, see instructions here.</p> <p>Please note, access via Blobfuse is planned but not yet implemented.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#install-docker","title":"Install Docker","text":"<p>It is possible to run this code without Docker, but at present we are only maintaining it via Docker Containers so we highly recommend installing Docker to run and/or test this code locally. Instructions are available for most operating systems here: https://docs.docker.com/desktop/</p>"},{"location":"install/#clone-repository-to-local-drive","title":"Clone repository to local drive","text":"<p>Run the following on a command line interface:</p> <pre><code>git clone git@github.com:Living-with-machines/lwmdb.git\ncd lwmdb\n</code></pre>"},{"location":"install/#running-locally","title":"Running locally","text":"usersudo <pre><code>docker compose -f local.yml up --build\n</code></pre> <pre><code>sudo docker compose -f local.yml up --build\n</code></pre> <p>Note</p> <p>This uses the <code>.envs/local</code> file provided in the repo. This must not be used in production, it is simply for local development and to ease demonstrating what is required for <code>.envs/production</code>, which must be generated separately for deploying via <code>production.yml</code>.</p> <p>It will take some time to download a set of <code>docker</code> images required to run locally, after which it should attempt to start the server in the <code>django</code> container. If successful, the console should print logs resembling</p> <pre><code>lwmdb_local_django    | WARNING: This is a development server. Do not use it in a production\ndeployment. Use a production WSGI server instead.\nlwmdb_local_django    |  * Running on all addresses (0.0.0.0)\nlwmdb_local_django    |  * Running on http://127.0.0.1:8000\nlwmdb_local_django    |  * Running on http://172.20.0.4:8000\nlwmdb_local_django    | Press CTRL+C to quit\nlwmdb_local_django    |  * Restarting with stat\nlwmdb_local_django    | Performing system checks...\nlwmdb_local_django    |\nlwmdb_local_django    | System check identified no issues (0 silenced).\nlwmdb_local_django    |\nlwmdb_local_django    | Django version 4.1.7, using settings 'lwmdb.settings'\nlwmdb_local_django    | Development server is running at http://0.0.0.0:8000/\nlwmdb_local_django    | Using the Werkzeug debugger (http://werkzeug.pocoo.org/)\nlwmdb_local_django    | Quit the server with CONTROL-C.\nlwmdb_local_django    |  * Debugger is active!\nlwmdb_local_django    |  * Debugger PIN: 139-826-693\n</code></pre> <p>Indicating it\u2019s up and running. You should then be able to go to <code>http://127.0.0.1:8000</code> in your local browser and see a start page.</p> <p>To stop the app call the <code>down</code> command:</p> usersudo <pre><code>docker compose -f local.yml down\n</code></pre> <pre><code>sudo docker compose -f local.yml down\n</code></pre>"},{"location":"install/#importing-data","title":"Importing data","text":"<p>If a previous version of the database is available as either <code>json</code> fixtures or raw <code>sql</code> via a <code>pg_dump</code> (or similar) command.</p>"},{"location":"install/#json-import","title":"<code>json</code> import","text":"<p><code>json</code> <code>fixtures</code> need to be placed in a <code>fixtures</code> folder in your local checkout:</p> <pre><code>cd lwmdb\nmkdir fixtures\ncp DataProvider-1.json  Ingest-1.json Item-1.json Newspaper-1.json Digitisation-1.json Issue-1.json Item-2.json fixtures/\n</code></pre> <p>The files can then be loaded into the <code>docker</code> container via</p> usersudo <pre><code>docker compose -f local.yml exec django /app/manage.py loaddata fixtures/Newspaper-1.json\ndocker compose -f local.yml exec django /app/manage.py loaddata fixtures/Issue-1.json\ndocker compose -f local.yml exec django /app/manage.py loaddata fixtures/Item-2.json\n...\n</code></pre> <pre><code>sudo docker compose -f local.yml exec django /app/manage.py loaddata fixtures/Newspaper-1.json\nsudo docker compose -f local.yml exec django /app/manage.py loaddata fixtures/Issue-1.json\nsudo docker compose -f local.yml exec django /app/manage.py loaddata fixtures/Item-2.json\n...\n</code></pre> <p>Warning</p> <p>Note the import order is important, specifically: <code>Newspaper</code>, <code>Issue</code> and any other data <code>json</code> files prior to <code>Item</code> <code>json</code>.</p>"},{"location":"install/#importing-a-postgres-database","title":"Importing a <code>postgres</code> database","text":"<p>Importing from <code>json</code> can be very slow. If provided a <code>postgres</code> data file, it is possible to import that directly. First copy the database file(s) to a <code>backups</code> folder on the <code>postgres</code> instance (assuming you\u2019ve run the <code>build</code> command)</p> usersudo <pre><code>docker cp backups/. $(docker compose -f local.yml ps -q postgres):backups\n</code></pre> <pre><code>sudo docker cp backups/. $(sudo docker compose -f local.yml ps -q postgres):backups\n</code></pre> <p>The available backups can be checked with</p> usersudo <pre><code>docker compose -f local.yml exec postgres backups\n</code></pre> <pre><code>sudo docker compose -f local.yml exec postgres backups\n</code></pre> <p>listing files in the desired folder:</p> <pre><code>sudo docker compose -f local.yml exec postgres ls backups\n</code></pre> <p>Next make sure the app is shut down, then start up with only the <code>postgres</code> container running:</p> usersudo <pre><code>docker compose -f local.yml down\ndocker compose -f local.yml up postgres\n</code></pre> <pre><code>sudo docker compose -f local.yml down\nsudo docker compose -f local.yml up postgres\n</code></pre> <p>Then run the <code>restore</code> command with the filename of the backup. By default backup filenames indicates when the backup was made and are compressed (using <code>gzip</code> compression in the example below <code>backup_2023_04_03T07_22_10.sql.gz</code>):</p> <p>Warning</p> <p>There is a chance the default <code>docker</code> size allocated is not big enough for a full version of the dataset (especially if running on a desktop). If so, you may need to increase the allocated disk space. For example, see <code>Docker Mac FAQs</code> for instructions to increase available disk space.</p> usersudo <pre><code>docker compose -f local.yml exec postgres restore backup_2023_04_03T07_22_10.sql.gz\n</code></pre> <pre><code>sudo docker compose -f local.yml exec postgres restore backup_2023_04_03T07_22_10.sql.gz\n</code></pre> <p>Warning</p> <p>If the version of the database you are loading is not compatible with the current version of the python package, this can cause significant errors</p>"},{"location":"install/#upgrade-development-version","title":"Upgrade development version","text":"<p>In order to upgrade the current development version that you have, make sure that you have synchronised the repository to your local drive:</p> usersudo <pre><code>git pull\ndocker compose -f local.yml up --build\n</code></pre> <pre><code>git pull\nsudo docker compose -f local.yml up --build\n</code></pre>"},{"location":"jupyter_notebook/","title":"Jupyter","text":""},{"location":"jupyter_notebook/#querying-the-database","title":"Querying the database","text":""},{"location":"jupyter_notebook/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>In order to run the Django framework inside a notebook, open another terminal window once you have it running via <code>docker</code> as described above and run</p> usersudo <pre><code>docker compose -f local.yml exec django /app/manage.py shell_plus --notebook\n</code></pre> <pre><code>sudo docker compose -f local.yml exec django /app/manage.py shell_plus --notebook\n</code></pre> <p>This should launch a normal Jupyter Notebook in your browser window where you can create any notebooks and access the database in different ways.</p> <p>Important</p> <p>Before importing any models and working with the database data, you will want to run the <code>import django_initialiser</code> in a cell, which will set up all the dependencies needed.</p> <p>The package comes with a <code>getting-started.ipynb</code> notebook and a <code>explore-newspapers.ipynb</code> notebook, which both will give some overview of how one can access the database\u2019s information and what one can do with it. They only scratch the surface of what is possible, of course, but will be a good entry point for someone who wants to orient themselves toward the database and the Django syntax for querying.</p>"},{"location":"management/","title":"Server Management","text":"<p>Much of the configuration in this project is derived from <code>django-cookiecutter</code>, and the information below is derived from their execllent documentation and avoid ambiguities between updates to their configuration and this project.</p> <p>Note</p> <p>The viability of this work is very much dependent on the great detail and sophistication of that project, and we are greatful for that work, and the work of the many open source projects we have priviledged to work with and use.</p>"},{"location":"management/#managing-data","title":"Managing Data","text":"<p>Managing versions of the datasets included in this project is quite complex, and can take significant time. We advises reading through all the instructions below before running any of the commands provided, and ensuring you have a version consistent with what you have installed locally and run.</p>"},{"location":"management/#backing-up","title":"Backing up","text":"<p>It is helpful to save the database in a state prior to, for example, redepoloying or a refactor. To do this run</p> usersudo <pre><code>docker compose -f local.yml exec postgres backup\nBacking up the 'lwmdb' database...\nSUCCESS: 'lwmdb' database backup 'backup_2023_05_17T12_38_40.sql.gz' has been created and placed in '/backups'.\n</code></pre> <pre><code>sudo docker compose -f local.yml exec postgres backup\nBacking up the 'lwmdb' database...\nSUCCESS: 'lwmdb' database backup 'backup_2023_05_17T12_38_40.sql.gz' has been created and placed in '/backups'.\n</code></pre> <p>Which generates the a backup file (<code>backup_2023_05_17T12_38_40.sql.gz</code> in this case) in the <code>/backups</code> folder of the <code>postgres</code> <code>docker</code> container.</p> <p>This can take some time, but will greate a compressed, complete version of the database, including login information, that can then be applied if something breaks or a new deploy on a new computer is needed.</p>"},{"location":"management/#listing-database-backups","title":"Listing database backups","text":"<p>Assuming default configurations, all database backup files will be in <code>/backups/</code> in a local <code>postgres</code> <code>docker</code> <code>container</code>. To copy that file out, it is helpful to check which backups are available</p> usersudo <pre><code>docker compose -f local.yml exec postgres backups\nThese are the backups you have got:\ntotal 27G\n-rw-r--r-- 1 root root 9.3G May 17 13:14 backup_2023_05_17T12_38_40.sql.gz\n-rw-r--r-- 1 root root 9.3G Apr 18 19:15 backup_2023_04_18T18_41_00.sql.gz\n-rw-r--r-- 1 root root 8.0G Apr  3 07:51 backup_2023_04_03T07_22_10.sql.gz\n</code></pre> <pre><code>sudo docker compose -f local.yml exec postgres backups\nThese are the backups you have got:\ntotal 27G\n-rw-r--r-- 1 root root 9.3G May 17 13:14 backup_2023_05_17T12_38_40.sql.gz\n-rw-r--r-- 1 root root 9.3G Apr 18 19:15 backup_2023_04_18T18_41_00.sql.gz\n-rw-r--r-- 1 root root 8.0G Apr  3 07:51 backup_2023_04_03T07_22_10.sql.gz\n</code></pre>"},{"location":"management/#copying-all-backup-files-out-of-docker","title":"Copying all backup files out of <code>docker</code>","text":"<p>Again assuming default configurations, to copy backup files out of the <code>postgres</code> <code>docker</code> <code>container</code> to a <code>backups</code> folder in the working directory of your local filesystem (assuming logged in as <code>user</code>): </p> usersudo <pre><code>docker cp $(docker compose -f local.yml ps -q postgres):backups backups\nSuccessfully copied 28.5GB to /home/user/lwmdb/backups\n</code></pre> <pre><code>sudo docker cp $(sudo docker compose -f local.yml ps -q postgres):backups backups\nSuccessfully copied 28.5GB to /home/user/lwmdb/backups\n</code></pre>"},{"location":"management/#copying-one-backup-file-out-of-docker","title":"Copying one backup file out of <code>docker</code>","text":"<p>To simply copy one backup (<code>backup_2023_05_17T12_38_40.sql.gz</code> in this case) database file (rather than all) to a local <code>backups</code> folder</p> usersudo <pre><code>docker cp $(docker compose -f local.yml ps -q postgres):backups/backup_2023_05_17T12_38_40.sql.gz backups/\n</code></pre> <pre><code>sudo docker cp $(sudo docker compose -f local.yml ps -q postgres):backups/backup_2023_05_17T12_38_40.sql.gz backups/\n</code></pre>"},{"location":"management/#database-configuration","title":"Database configuration","text":"<p>It is possible to name the database something other thatn <code>lwmdb</code> if needed, and that can be helpful if needed to make a schema change and wanting to copy the schema to manage that process. </p> <p>The name of the database schema is specified in local <code>.env/local</code> and <code>.env/production</code> <code>env</code> files. The default <code>.env/local</code> includes</p> <pre><code>POSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=lwmdb\nPOSTGRES_USER=lwmdb\n...\n</code></pre> <p>The name of the database is separate from the names of the tables so, for example, you can replace the <code>POSTGRES_DB</code> and <code>POSTGRES_USER</code> with your preferred configuration. This works because each of the models in this project (for example <code>newspapers.Item</code> )</p> <p>Warning</p> <p>It is strongly advised to keep <code>.env/production</code> and <code>.env/local</code> database configurations the same for testing purposes. </p>"},{"location":"raw_import/","title":"Import","text":""},{"location":"raw_import/#order-of-import","title":"Order of import","text":""},{"location":"raw_import/#newspapers","title":"Newspapers","text":""},{"location":"raw_import/#post-newspapers","title":"Post Newspapers","text":"<p>Note</p> <p>It may be possible to run some alongside each other like:</p> <pre><code>python manage.py createfixtures gazetteer mitchells\n</code></pre> <p>General workflow -&gt; process data to <code>json</code>, then import as fixtures</p> <p>Output path for fixture loading</p> <pre><code>settings.BASE_DIR / Path(f\"{app_name}/fixtures\")\n</code></pre>"},{"location":"raw_import/#gazzetteers","title":"Gazzetteers","text":"<p>Generates <code>.json files</code></p> <pre><code>python manage.py createfixtures gazetteer\n</code></pre> <p>Result from gazetteer <code>createfixtures</code> script: </p> <ul> <li><code>HistoricCounty-fixtures.json</code></li> <li><code>AdminCounty-fixtures.json</code></li> <li><code>Place-fixtures.json</code></li> </ul>"},{"location":"raw_import/#mitchells","title":"Mitchells","text":"<ul> <li><code>EntryPoliticalLeanings-fixtures.json</code></li> <li><code>EntryPrices-fixtures.json</code></li> <li><code>Issue-fixtures.json</code></li> <li><code>PoliticalLeaning-fixtures.json</code></li> <li><code>Price-fixtures.json</code></li> <li><code>Entry-fixtures.json</code></li> </ul>"},{"location":"raw_import/#census","title":"Census","text":"<p>Connects to gazetteer records</p> <p>Is more of a script, it doesn\u2019t address process of <code>json</code> exports.</p> <ul> <li>NOT public</li> <li>WGS84: https://gisgeography.com/wgs84-world-geodetic-system/</li> <li>Wikidata coordinate system</li> <li>Ask Mariona</li> </ul>"},{"location":"schema/","title":"Database Schema","text":"A full <code>kroki</code>-generated schema to check discrepensies with code and above <pre><code>@from_file:assets/sql_structure.dot\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting: Common issues","text":""},{"location":"troubleshooting/#deploy-using-up-very-large-harddrive-space","title":"Deploy using up very large harddrive space","text":"<p>By default, a <code>docker</code> deploy copies most files from the folder the project is run from to each <code>docker</code> <code>container</code>. However, this can lead to very large <code>container</code> images if, for example, a recent database backup has been saved locally. To mitigate this, the <code>.dockerignore</code> file provided currently excludes file paths that match these patterns (as of 4 May 2023, see ticket #90 for the latest on this issue):</p> <pre><code>fixtures/*\nbackups*\n*.tar\n*.sql.gz\n</code></pre> <p>It also has a line for a file to include (see the <code>.dockerfile</code> docs for syntax):</p> <pre><code>!fixture-files/*\n</code></pre> <p>If you\u2019ve added another very large file that may have ended up taking up a lot of space on your deploys (and potentially a very long  deploy time) adding a lines to avoid those files in <code>.dockerignore</code> may help. For that to take effect, you may need to do a fresh build without the cache.</p> usersudo <pre><code>docker compose -f local.yml build --no-cache\n</code></pre> <p><pre><code>sudo docker compose -f local.yml build --no-cache\n</code></pre> ```</p>"},{"location":"troubleshooting/#error-improperlyconfigured-requested-setting-installed_apps-but-settings-are-not-configured","title":"<code>Error: ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not configured.</code>","text":"<p>Problem: I have received an error that looks like this:</p> <p>ImproperlyConfigured\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Traceback (most recent call last) \u2026 ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.</p> <p>Explanation: You have likely attempted to import any of the models (<code>Newspaper</code>, <code>Item</code>, <code>Entry</code>, etc.) and forgotten about the <code>import django_initialiser</code> statement that is required to set up Django in a Jupyter Notebook.</p> <p>Solution: You must run <code>import django_initialiser</code> before you attempt to import any models from the Django package.</p> <p>If it does not work: Are you running the notebook in the same folder as the <code>manage.py</code> script? Otherwise, try to move the notebook to that folder.</p>"},{"location":"troubleshooting/#nameerror-name-newspaper-is-not-defined","title":"<code>NameError: name 'Newspaper' is not defined</code>","text":"<p>Problem: I have received an error that looks like this:</p> <p>NameError\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Traceback (most recent call last) \u2026 NameError: name \u2018Newspaper\u2019 is not defined</p> <p>Explanation: You have likely forgotten to import the correct model before you tried to run a query on one of the newspapers (or whichever model you\u2019re trying to access).</p> <p>Solution: Run <code>from newspapers.models import Newspaper</code> or follow the same pattern for whichever model you want to import. (See the database schema if you are unsure which model you want to access.)</p> <p>If it does not work: Are you running the notebook in the same folder as the <code>manage.py</code> script? Otherwise, try to move the notebook to that folder.</p>"},{"location":"reference/DOC_STRINGS/","title":"DOC STRINGS","text":"<ul> <li>census<ul> <li>admin</li> <li>apps</li> <li>management<ul> <li>commands<ul> <li>census</li> </ul> </li> </ul> </li> <li>migrations</li> <li>models</li> <li>views</li> </ul> </li> <li>config<ul> <li>asgi</li> <li>production</li> <li>settings</li> <li>test_settings</li> <li>urls</li> <li>wsgi</li> </ul> </li> <li>conftest</li> <li>django_initialiser</li> <li>fulltext<ul> <li>admin</li> <li>apps</li> <li>migrations</li> <li>models</li> <li>views</li> </ul> </li> <li>gazetteer<ul> <li>admin</li> <li>apps</li> <li>management<ul> <li>commands<ul> <li>gazetteer</li> </ul> </li> </ul> </li> <li>migrations</li> <li>models</li> <li>views</li> </ul> </li> <li>lwmdb<ul> <li>contrib<ul> <li>sites<ul> <li>migrations</li> </ul> </li> </ul> </li> <li>management<ul> <li>commands<ul> <li>connect</li> <li>createfixtures</li> <li>fixtures</li> <li>load_json_fixtures</li> <li>loadfixtures</li> <li>makeitemfixtures</li> </ul> </li> </ul> </li> <li>tests</li> <li>utils</li> </ul> </li> <li>mitchells<ul> <li>admin</li> <li>apps</li> <li>import_fixtures</li> <li>migrations</li> <li>models</li> <li>views</li> </ul> </li> <li>newspapers<ul> <li>admin</li> <li>apps</li> <li>management<ul> <li>commands<ul> <li>items</li> <li>newspapers</li> </ul> </li> </ul> </li> <li>migrations</li> <li>models</li> <li>views</li> </ul> </li> </ul>"},{"location":"reference/conftest/","title":"conftest","text":""},{"location":"reference/conftest/#conftest.media_storage","title":"media_storage","text":"<pre><code>media_storage(settings, tmpdir) -&gt; None\n</code></pre> <p>Generate a temp path for testing media files.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture(autouse=True)\ndef media_storage(settings, tmpdir) -&gt; None:\n    \"\"\"Generate a temp path for testing media files.\"\"\"\n    settings.MEDIA_ROOT = tmpdir.strpath\n</code></pre>"},{"location":"reference/conftest/#conftest.mitchells_data_path","title":"mitchells_data_path","text":"<pre><code>mitchells_data_path() -&gt; Path\n</code></pre> <p>Return path to <code>mitchells</code> app data.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef mitchells_data_path() -&gt; Path:\n    \"\"\"Return path to `mitchells` app data.\"\"\"\n    return app_data_path(\"mitchells\") / MITCHELLS_EXCEL_PATH\n</code></pre>"},{"location":"reference/conftest/#conftest.old_data_provider","title":"old_data_provider","text":"<pre><code>old_data_provider(old_data_provider_fixture_path: Path) -&gt; None\n</code></pre> <p>Load old example <code>newspaper.DataProvider</code> fixture.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture\n@pytest.mark.django_db\ndef old_data_provider(old_data_provider_fixture_path: Path) -&gt; None:\n    \"\"\"Load old example `newspaper.DataProvider` fixture.\"\"\"\n    call_command(\"loaddata\", old_data_provider_fixture_path)\n</code></pre>"},{"location":"reference/conftest/#conftest.old_data_provider_fixture_path","title":"old_data_provider_fixture_path","text":"<pre><code>old_data_provider_fixture_path() -&gt; Path\n</code></pre> <p>Load old example <code>newspaper.DataProvider</code> fixture.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture\ndef old_data_provider_fixture_path() -&gt; Path:\n    \"\"\"Load old example `newspaper.DataProvider` fixture.\"\"\"\n    return Path(\"lwmdb/tests/initial-test-dataprovider.json\")\n</code></pre>"},{"location":"reference/conftest/#conftest.pytest_sessionfinish","title":"pytest_sessionfinish","text":"<pre><code>pytest_sessionfinish(session, exitstatus)\n</code></pre> <p>Generate badges for docs after tests finish.</p> Source code in <code>conftest.py</code> <pre><code>def pytest_sessionfinish(session, exitstatus):\n    \"\"\"Generate badges for docs after tests finish.\"\"\"\n    if exitstatus == 0:\n        BADGE_PATH.parent.mkdir(parents=True, exist_ok=True)\n        gen_cov_badge([\"-o\", f\"{BADGE_PATH}\", \"-f\"])\n</code></pre>"},{"location":"reference/conftest/#conftest.set_default_language","title":"set_default_language","text":"<pre><code>set_default_language() -&gt; None\n</code></pre> <p>Ensure <code>en-gb</code> localisation is enforced for testing.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture(autouse=True)\ndef set_default_language() -&gt; None:\n    \"\"\"Ensure `en-gb` localisation is enforced for testing.\"\"\"\n    activate(\"en-gb\")\n</code></pre>"},{"location":"reference/conftest/#conftest.updated_data_provider_path","title":"updated_data_provider_path","text":"<pre><code>updated_data_provider_path() -&gt; Path\n</code></pre> <p>Load old example <code>newspaper.DataProvider</code> fixture.</p> Source code in <code>conftest.py</code> <pre><code>@pytest.fixture\ndef updated_data_provider_path() -&gt; Path:\n    \"\"\"Load old example `newspaper.DataProvider` fixture.\"\"\"\n    return Path(\"lwmdb/tests/update-test-dataprovider.json\")\n</code></pre>"},{"location":"reference/census/","title":"census","text":""},{"location":"reference/census/admin/","title":"admin","text":""},{"location":"reference/census/apps/","title":"apps","text":""},{"location":"reference/census/models/","title":"models","text":""},{"location":"reference/census/views/","title":"views","text":""},{"location":"reference/census/management/","title":"management","text":""},{"location":"reference/census/management/commands/","title":"commands","text":""},{"location":"reference/census/management/commands/census/","title":"census","text":""},{"location":"reference/census/management/commands/census/#census.management.commands.census.Command","title":"Command","text":"<p>             Bases: <code>Fixture</code></p> <p>Build census.</p> Source code in <code>census/management/commands/census.py</code> <pre><code>class Command(Fixture):\n    \"\"\"Build census.\"\"\"\n\n    csv_fixture_path: Path = CSV_FIXTURE_PATH\n    json_fixture_write_path: Path = JSON_FIXTURE_WRITE_PATH\n\n    def __init__(self, force=False):\n        self.force = force\n        super(Fixture, self).__init__()\n\n    def build_fixture(self):\n        CSV_FILES = [x for x in self.csv_fixture_path.glob(\"*.csv\")]\n\n        df = pd.DataFrame()\n        now = timezone.now()\n\n        for file in (bar1 := tqdm(CSV_FILES, leave=False)):\n            bar1.set_description(file.name)\n\n            year, *_ = re.findall(r\"\\d{4}\", str(file.name))\n            _df = pd.read_csv(file)\n            _df = _df.rename({f\"CEN_{year}\": \"CEN\"}, axis=1)\n            _df[\"CENSUS_YEAR\"] = year\n            _df = _df[\n                [\"CENSUS_YEAR\"] + [x for x in _df.columns if not x == \"CENSUS_YEAR\"]\n            ]\n\n            df = pd.concat([df, _df])\n\n        df = df.rename(\n            {col: col.replace(\"-\", \"_\") for col in df.columns if \"-\" in col},\n            axis=1,\n        )\n        df = df.reset_index(drop=True)\n        df[\"pk\"] = df.index + 1\n        df[\"created_at\"] = str(now)\n        df[\"updated_at\"] = str(now)\n\n        def get_rel(x):\n            # do some manual data wrangling - on hold until Mariona is back\n            if \"yorkshire\" in x.lower():\n                x = \"YORKSHIRE\"\n\n            if \"bury\" in x and \"edmund\" in x.lower():\n                x = \"Bury St Edmunds\"\n\n            #####\u00a0first try\n            try:\n                historic_county = HistoricCounty.objects.get(label__iexact=x).pk\n            except:\n                historic_county = None\n\n            try:\n                admin_county = AdminCounty.objects.get(label__iexact=x).pk\n            except:\n                admin_county = None\n\n            try:\n                place = Place.objects.get(label__iexact=x).pk\n            except:\n                place = None\n\n            if historic_county != None or admin_county != None or place != None:\n                return (x, historic_county, admin_county, place)\n\n            ##### second try: without EAST/WEST/NORTH/SOUTH/WESTERN/SOUTHEAST/FIRST/CENTRAL\n            if historic_county == None and place == None:\n                for word in [\n                    \"east\",\n                    \"north\",\n                    \"west\",\n                    \"south\",\n                    \"western\",\n                    \"southeast\",\n                    \"first\",\n                    \"central\",\n                    \"south east\",\n                ]:\n                    if f\"{word} \" in x.lower() or f\" {word}\" in x.lower():\n                        x = x.replace(f\"{word} \", \" \").replace(f\" {word}\", \" \").strip()\n                        x = (\n                            x.replace(f\"{word.upper()} \", \" \")\n                            .replace(f\" {word.upper()}\", \" \")\n                            .strip()\n                        )\n\n            try:\n                historic_county = HistoricCounty.objects.get(label__iexact=x).pk\n            except:\n                historic_county = None\n\n            try:\n                admin_county = AdminCounty.objects.get(label__iexact=x).pk\n            except:\n                admin_county = None\n\n            try:\n                place = Place.objects.get(label__iexact=x).pk\n            except:\n                place = None\n\n            return (x, historic_county, admin_county, place)\n\n        cats = [\"REGCNTY\", \"REGDIST\", \"SUBDIST\"]\n\n        for cat in (bar1 := tqdm(cats, leave=False)):\n            bar1.set_description(f\"Correcting record :: {cat}\")\n            df[f\"{cat}_rel\"] = df[cat].apply(lambda x: get_rel(x))\n            df[f\"{cat}_historic_county_id\"] = df[f\"{cat}_rel\"].apply(lambda x: x[1])\n            df[f\"{cat}_admin_county_id\"] = df[f\"{cat}_rel\"].apply(lambda x: x[2])\n            df[f\"{cat}_place_id\"] = df[f\"{cat}_rel\"].apply(lambda x: x[3])\n\n        df.drop([f\"{cat}_rel\" for cat in cats], axis=1, inplace=True)\n\n        lst = []\n        for record in json.loads(df.to_json(orient=\"records\")):\n            pk = record.pop(\"pk\")\n            lst.append({\"model\": \"census.Record\", \"pk\": pk, \"fields\": record})\n\n        self.json_fixture_write_path.write_text(json.dumps(lst))\n\n        self.stdout.write(\n            self.style.SUCCESS(\"Fixture file written. Now run the following command:\")\n        )\n        self.stdout.write(\n            self.style.SUCCESS(\n                f\"python manage.py loaddata {self.json_fixture_write_path}\"\n            )\n        )\n</code></pre>"},{"location":"reference/census/migrations/","title":"migrations","text":""},{"location":"reference/config/","title":"config","text":""},{"location":"reference/config/asgi/","title":"asgi","text":"<p>ASGI config for lwmdb project.</p> <p>It exposes the ASGI callable as a module-level variable named <code>application</code>.</p> <p>For more information on this file, see https://docs.djangoproject.com/en/4.2/howto/deployment/asgi/</p>"},{"location":"reference/config/production/","title":"production","text":""},{"location":"reference/config/settings/","title":"settings","text":"<p>Django settings for lwmdb project.</p> <p>Generated by \u2018django-admin startproject\u2019 using Django 4.0. For more information on this file, see</p> <p>https://docs.djangoproject.com/en/4.0/topics/settings/ https://docs.djangoproject.com/en/4.0/ref/settings/</p>"},{"location":"reference/config/settings/#config.settings.is_docker","title":"is_docker","text":"<pre><code>is_docker()\n</code></pre> <p>Test if currently running in Docker.</p> <p>See https://stackoverflow.com/questions/43878953/how-does-one- detect-if-one-is-running-within-a-docker-container-within-python</p> Source code in <code>config/settings.py</code> <pre><code>def is_docker():\n    \"\"\"Test if currently running in Docker.\n\n    See https://stackoverflow.com/questions/43878953/how-does-one-\n    detect-if-one-is-running-within-a-docker-container-within-python\n    \"\"\"\n    cgroup = Path(\"/proc/self/cgroup\")\n    return (\n        Path(\"/.dockerenv\").is_file()\n        or cgroup.is_file()\n        and cgroup.read_text().find(\"docker\") &gt; -1\n    )\n</code></pre>"},{"location":"reference/config/test_settings/","title":"test_settings","text":"<p>With these settings, tests run faster.</p>"},{"location":"reference/config/urls/","title":"urls","text":"<p>Metadata URL Configuration.</p> <p>The <code>urlpatterns</code> list routes URLs to views. For more information please see:     https://docs.djangoproject.com/en/4.0/topics/http/urls/</p> <p>Examples:</p> <p>Function views     1. Add an import:  from my_app import views     2. Add a URL to urlpatterns:  path(\u2018\u2019, views.home, name=\u2019home\u2019) Class-based views     1. Add an import:  from other_app.views import Home     2. Add a URL to urlpatterns:  path(\u2018\u2019, Home.as_view(), name=\u2019home\u2019) Including another URLconf     1. Import the include() function: from django.urls import include, path     2. Add a URL to urlpatterns:  path(\u2018blog/\u2019, include(\u2018blog.urls\u2019))</p>"},{"location":"reference/config/wsgi/","title":"wsgi","text":"<p>WSGI config for lwmdb project.</p> <p>It exposes the WSGI callable as a module-level variable named <code>application</code>.</p> <p>For more information on this file, see https://docs.djangoproject.com/en/4.0/howto/deployment/wsgi/</p>"},{"location":"reference/django_initialiser/","title":"django_initialiser","text":""},{"location":"reference/fulltext/","title":"fulltext","text":""},{"location":"reference/fulltext/admin/","title":"admin","text":""},{"location":"reference/fulltext/apps/","title":"apps","text":""},{"location":"reference/fulltext/models/","title":"models","text":""},{"location":"reference/fulltext/views/","title":"views","text":""},{"location":"reference/fulltext/migrations/","title":"migrations","text":""},{"location":"reference/gazetteer/","title":"gazetteer","text":""},{"location":"reference/gazetteer/admin/","title":"admin","text":""},{"location":"reference/gazetteer/apps/","title":"apps","text":""},{"location":"reference/gazetteer/models/","title":"models","text":""},{"location":"reference/gazetteer/views/","title":"views","text":""},{"location":"reference/gazetteer/management/","title":"management","text":""},{"location":"reference/gazetteer/management/commands/","title":"commands","text":""},{"location":"reference/gazetteer/management/commands/gazetteer/","title":"gazetteer","text":""},{"location":"reference/gazetteer/migrations/","title":"migrations","text":""},{"location":"reference/lwmdb/","title":"lwmdb","text":""},{"location":"reference/lwmdb/utils/","title":"utils","text":""},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource","title":"DataSource  <code>dataclass</code>","text":"<p>Class to manage storing/deleting data files.</p> Example <pre><code>&gt;&gt;&gt; import census\n&gt;&gt;&gt; from pandas import read_csv\n\n&gt;&gt;&gt; rsd_1851: DataSource = DataSource(\n...     file_name=\"demographics_england_wales_2015.csv\",\n...     app=census,\n...     url=\"https://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv\",\n...     read_func=read_csv,\n...     description=\"Demographic and socio-economic variables for Registration Sub-Districts (RSDs) in England and Wales, 1851\",\n...     citation=\"https://dx.doi.org/10.5255/UKDA-SN-853547\",\n...     license=\"http://creativecommons.org/licenses/by/4.0/\",\n... )\n&gt;&gt;&gt; rsd_1851.delete()  # To ensure it passes tests and doesn't fail\n&gt;&gt;&gt; df = rsd_1851.read()\ncensus/data/demographics_england_wales_2015.csv not found, downloading from https://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv\nhttps://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv file available from census/data/demographics_england_wales_2015.csv\n&gt;&gt;&gt; df.columns[:5].tolist()\n['CEN_1851', 'REGCNTY', 'REGDIST', 'SUBDIST', 'POP_DENS']\n&gt;&gt;&gt; rsd_1851.delete()\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>@dataclass\nclass DataSource:\n    \"\"\"Class to manage storing/deleting data files.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; import census\n        &gt;&gt;&gt; from pandas import read_csv\n\n        &gt;&gt;&gt; rsd_1851: DataSource = DataSource(\n        ...     file_name=\"demographics_england_wales_2015.csv\",\n        ...     app=census,\n        ...     url=\"https://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv\",\n        ...     read_func=read_csv,\n        ...     description=\"Demographic and socio-economic variables for Registration Sub-Districts (RSDs) in England and Wales, 1851\",\n        ...     citation=\"https://dx.doi.org/10.5255/UKDA-SN-853547\",\n        ...     license=\"http://creativecommons.org/licenses/by/4.0/\",\n        ... )\n        &gt;&gt;&gt; rsd_1851.delete()  # To ensure it passes tests and doesn't fail\n        &gt;&gt;&gt; df = rsd_1851.read()\n        census/data/demographics_england_wales_2015.csv not found, downloading from https://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv\n        https://reshare.ukdataservice.ac.uk/853547/4/1851_RSD_data.csv file available from census/data/demographics_england_wales_2015.csv\n        &gt;&gt;&gt; df.columns[:5].tolist()\n        ['CEN_1851', 'REGCNTY', 'REGDIST', 'SUBDIST', 'POP_DENS']\n        &gt;&gt;&gt; rsd_1851.delete()\n\n        ```\n    \"\"\"\n\n    file_name: PathLike | str\n    app: ModuleType\n    url: str\n    read_func: Callable[[PathLike], DataFrame | Series]\n    description: str | None = None\n    citation: str | None = None\n    license: str | None = None\n    _download_exception: DataSourceDownloadError | None = None\n    _str_truncation_length: int = 15\n\n    def __str__(self) -&gt; str:\n        \"\"\"Readable description of which `file_name` from which `app`.\"\"\"\n        return (\n            f\"'{_short_text_trunc(str(self.file_name))}' \"\n            f\"for `{self.app.__name__}` app data\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed, truncated reprt of `file_name` for `app`.\"\"\"\n        return (\n            f\"{self.__class__.__name__}({self.app.__name__!r}, \"\n            f\"'{_short_text_trunc(str(self.file_name))}')\"\n        )\n\n    @property\n    def url_suffix(self) -&gt; str:\n        \"\"\"Return suffix of `self.url` or None if not found.\"\"\"\n        return path_or_str_suffix(self.url)\n\n    @property\n    def _trunc_url_str_suffix(self) -&gt; str:\n        \"\"\"Return DEFAULT_TRUNCATION_CHARS + `self.url_suffix`.\"\"\"\n        return DEFAULT_TRUNCATION_CHARS + self.url_suffix\n\n    def _file_name_truncated(self) -&gt; str:\n        \"\"\"Return truncated `file_name` for logging.\"\"\"\n        return truncate_str(\n            text=Path(self.file_name).suffix,\n            max_length=self._str_truncation_length,\n            trail_str=self._trunc_url_str_suffix,\n        )\n\n    @property\n    def local_path(self) -&gt; Path:\n        \"\"\"Return path to store `self.file_name`.\"\"\"\n        return app_data_path(self.app.__name__) / self.file_name\n\n    @property\n    def is_empty(self) -&gt; bool:\n        \"\"\"Return if `Path` to store `self.file_name` has 0 file size.\"\"\"\n        return self.local_path.stat().st_size == 0\n\n    @property\n    def is_file(self) -&gt; bool:\n        \"\"\"Return if `self.local_path` is a file.\"\"\"\n        return self.local_path.is_file()\n\n    @property\n    def is_local(self) -&gt; bool:\n        \"\"\"Return if `self.url` is storred locally at `self.file_name`.\"\"\"\n        return self.is_file and not self.is_empty\n\n    def download(self, force: bool = False) -&gt; bool:\n        \"\"\"Download `self.url` to save locally at `self.file_name`.\"\"\"\n        if self.is_local and not force:\n            log_and_django_terminal(\n                f\"{self} already downloaded \" f\"(add `force=True` to override)\"\n            )\n            return True\n        else:\n            return download_file(self.local_path, self.url)\n\n    def delete(self) -&gt; None:\n        \"\"\"Delete local save of `self.url` at `self.file_name`.\n\n        Note:\n            No error raised if missing.\n        \"\"\"\n        log_and_django_terminal(f\"Deleting local copy of {self} at {self.local_path}\")\n        self.local_path.unlink(missing_ok=True)\n\n    def read(self, force: bool = False) -&gt; DataFrame | Series:\n        \"\"\"Return data in `self.local_path` processed by `self.read_func`.\"\"\"\n        if not self.is_local:\n            success: bool = self.download(force=force)\n            if not success:\n                self._download_exception = DataSourceDownloadError(\n                    f\"Failed to access {self} data from {self.url}\"\n                )\n                logger.error(str(self._download_exception))\n        return self.read_func(self.local_path)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.is_empty","title":"is_empty  <code>property</code>","text":"<pre><code>is_empty: bool\n</code></pre> <p>Return if <code>Path</code> to store <code>self.file_name</code> has 0 file size.</p>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.is_file","title":"is_file  <code>property</code>","text":"<pre><code>is_file: bool\n</code></pre> <p>Return if <code>self.local_path</code> is a file.</p>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.is_local","title":"is_local  <code>property</code>","text":"<pre><code>is_local: bool\n</code></pre> <p>Return if <code>self.url</code> is storred locally at <code>self.file_name</code>.</p>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.local_path","title":"local_path  <code>property</code>","text":"<pre><code>local_path: Path\n</code></pre> <p>Return path to store <code>self.file_name</code>.</p>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.url_suffix","title":"url_suffix  <code>property</code>","text":"<pre><code>url_suffix: str\n</code></pre> <p>Return suffix of <code>self.url</code> or None if not found.</p>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Detailed, truncated reprt of <code>file_name</code> for <code>app</code>.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed, truncated reprt of `file_name` for `app`.\"\"\"\n    return (\n        f\"{self.__class__.__name__}({self.app.__name__!r}, \"\n        f\"'{_short_text_trunc(str(self.file_name))}')\"\n    )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Readable description of which <code>file_name</code> from which <code>app</code>.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Readable description of which `file_name` from which `app`.\"\"\"\n    return (\n        f\"'{_short_text_trunc(str(self.file_name))}' \"\n        f\"for `{self.app.__name__}` app data\"\n    )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Delete local save of <code>self.url</code> at <code>self.file_name</code>.</p> Note <p>No error raised if missing.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Delete local save of `self.url` at `self.file_name`.\n\n    Note:\n        No error raised if missing.\n    \"\"\"\n    log_and_django_terminal(f\"Deleting local copy of {self} at {self.local_path}\")\n    self.local_path.unlink(missing_ok=True)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.download","title":"download","text":"<pre><code>download(force: bool = False) -&gt; bool\n</code></pre> <p>Download <code>self.url</code> to save locally at <code>self.file_name</code>.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def download(self, force: bool = False) -&gt; bool:\n    \"\"\"Download `self.url` to save locally at `self.file_name`.\"\"\"\n    if self.is_local and not force:\n        log_and_django_terminal(\n            f\"{self} already downloaded \" f\"(add `force=True` to override)\"\n        )\n        return True\n    else:\n        return download_file(self.local_path, self.url)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.DataSource.read","title":"read","text":"<pre><code>read(force: bool = False) -&gt; DataFrame | Series\n</code></pre> <p>Return data in <code>self.local_path</code> processed by <code>self.read_func</code>.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def read(self, force: bool = False) -&gt; DataFrame | Series:\n    \"\"\"Return data in `self.local_path` processed by `self.read_func`.\"\"\"\n    if not self.is_local:\n        success: bool = self.download(force=force)\n        if not success:\n            self._download_exception = DataSourceDownloadError(\n                f\"Failed to access {self} data from {self.url}\"\n            )\n            logger.error(str(self._download_exception))\n    return self.read_func(self.local_path)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.JSONFixtureType","title":"JSONFixtureType","text":"<p>             Bases: <code>TypedDict</code></p> <p>A type to check <code>JSON</code> fixture structure.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>class JSONFixtureType(TypedDict):\n    \"\"\"A type to check `JSON` fixture structure.\"\"\"\n\n    pk: str\n    model: str\n    fields: dict[str, Any]\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.app_data_path","title":"app_data_path","text":"<pre><code>app_data_path(\n    app_name: str, data_path: PathLike = DEFAULT_APP_DATA_FOLDER\n) -&gt; Path\n</code></pre> <p>Return <code>app_name</code> data <code>Path</code> and ensure exists.</p> Example <pre><code>&gt;&gt;&gt; app_data_path('mitchells')\nPosixPath('mitchells/data')\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def app_data_path(app_name: str, data_path: PathLike = DEFAULT_APP_DATA_FOLDER) -&gt; Path:\n    \"\"\"Return `app_name` data `Path` and ensure exists.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; app_data_path('mitchells')\n        PosixPath('mitchells/data')\n\n        ```\n    \"\"\"\n    path = Path(app_name) / Path(data_path)\n    path.mkdir(exist_ok=True)\n    return path\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.bulk_fixture_update","title":"bulk_fixture_update","text":"<pre><code>bulk_fixture_update(\n    fixture_path: PathLike,\n    create_new_records: bool = False,\n    batch_size: int = 1000,\n) -&gt; None\n</code></pre> <p>Modify existing records with fixture.</p> <p>Parameters:</p> Name Type Description Default <code>fixture_path</code> <code>PathLike</code> <p><code>Path</code> of fixture to update from</p> required <code>create_new_records</code> <code>bool</code> <p>Whether to add new records as well as updated fixtures</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Size for batch record updates (not applied to new records)</p> <code>1000</code> Example <pre><code>&gt;&gt;&gt; getfixture(\"db\")\n&gt;&gt;&gt; getfixture(\"old_data_provider\")\nInstalled 4 object(s) from 1 fixture(s)\n&gt;&gt;&gt; updated_fixture_path = getfixture(\"updated_data_provider_path\")\n&gt;&gt;&gt; from newspapers.models import DataProvider\n&gt;&gt;&gt; for provider in DataProvider.objects.all():\n...     print(provider)\nbna\nhmd\njisc\nlwm\n&gt;&gt;&gt; bulk_fixture_update(fixture_path=updated_fixture_path)\n&gt;&gt;&gt; for provider in DataProvider.objects.all():\n...     print(provider)\nFindMyPast\nHeritage Made Digital\nJoint Information Systems Committee\nLiving with Machines\n&gt;&gt;&gt; bulk_fixture_update(fixture_path=updated_fixture_path,\n...                     create_new_records=True)\nInstalled 1 object(s) from 1 fixture(s)\n&gt;&gt;&gt; for provider in DataProvider.objects.all():\n...     print(provider)\nFindMyPast\nHeritage Made Digital\nJoint Information Systems Committee\nLiving with Machines\nExample New Provider\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def bulk_fixture_update(\n    fixture_path: PathLike,\n    create_new_records: bool = False,\n    batch_size: int = 1000,\n) -&gt; None:\n    \"\"\"Modify existing records with fixture.\n\n    Args:\n        fixture_path:\n            `Path` of fixture to update from\n        create_new_records:\n            Whether to add new records as well as updated fixtures\n        batch_size:\n            Size for batch record updates (not applied to new records)\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; getfixture(\"db\")\n        &gt;&gt;&gt; getfixture(\"old_data_provider\")\n        Installed 4 object(s) from 1 fixture(s)\n        &gt;&gt;&gt; updated_fixture_path = getfixture(\"updated_data_provider_path\")\n        &gt;&gt;&gt; from newspapers.models import DataProvider\n        &gt;&gt;&gt; for provider in DataProvider.objects.all():\n        ...     print(provider)\n        bna\n        hmd\n        jisc\n        lwm\n        &gt;&gt;&gt; bulk_fixture_update(fixture_path=updated_fixture_path)\n        &gt;&gt;&gt; for provider in DataProvider.objects.all():\n        ...     print(provider)\n        FindMyPast\n        Heritage Made Digital\n        Joint Information Systems Committee\n        Living with Machines\n        &gt;&gt;&gt; bulk_fixture_update(fixture_path=updated_fixture_path,\n        ...                     create_new_records=True)\n        Installed 1 object(s) from 1 fixture(s)\n        &gt;&gt;&gt; for provider in DataProvider.objects.all():\n        ...     print(provider)\n        FindMyPast\n        Heritage Made Digital\n        Joint Information Systems Committee\n        Living with Machines\n        Example New Provider\n\n        ```\n    \"\"\"\n    records_to_update: defaultdict[type[Model], ModelUpdateDict] = defaultdict(\n        lambda: ModelUpdateDict(instances=[], fields=set()),\n    )\n    records_to_create: list[JSONFixtureType] = []\n    with open(fixture_path) as fixture:\n        record_dict: JSONFixtureType\n        for record_dict in json.loads(fixture.read()):\n            model_type: Model = apps.get_model(record_dict[\"model\"])\n            model_instance: Model | None = model_type.objects.filter(\n                pk=record_dict[\"pk\"]\n            ).first()\n            if model_instance:\n                for field, value in record_dict[\"fields\"].items():\n                    setattr(model_instance, field, value)\n                    records_to_update[model_type][\"fields\"].add(field)\n                records_to_update[model_type][\"instances\"].append(model_instance)\n            else:\n                records_to_create.append(record_dict)\n    updated_records: dict[type[Model], list[Model]] = {}\n    model: Model\n    updates_dict: ModelUpdateDict\n    for model, updates_dict in records_to_update.items():\n        log_and_django_terminal(\n            f\"Bulk updating {len(updates_dict['instances'])} {model} instances\\n\"\n            f\"Fields to update: {updates_dict['fields']}\",\n        )\n        updated_records[model] = model.objects.bulk_update(\n            updates_dict[\"instances\"], updates_dict[\"fields\"], batch_size=batch_size\n        )\n    if create_new_records:\n        with NamedTemporaryFile(mode=\"w\", suffix=\".json\") as new_fixtures_tempfile:\n            json.dump(records_to_create, fp=new_fixtures_tempfile)\n            new_fixtures_tempfile.flush()\n            call_command(\"loaddata\", new_fixtures_tempfile.name)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.callable_on_chunks","title":"callable_on_chunks","text":"<pre><code>callable_on_chunks(\n    qs: QuerySet,\n    method_name: str = DEFAULT_CALLABLE_CHUNK_METHOD_NAME,\n    start_index: int | None = None,\n    end_index: int | None = None,\n    chunk_size: int = DEFAULT_CALLABLE_CHUNK_SIZE,\n    terminal_print: bool = False,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Apply <code>method_name</code> to <code>qs</code>, filter by <code>start_index</code> and <code>end_index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>qs</code> <code>QuerySet</code> <p><code>django</code> <code>QerySet</code> instance to apply <code>method_name</code> total</p> required <code>method_name</code> <code>str</code> <p><code>Callable</code> <code>method</code> of <code>qs</code> <code>class</code> to apply to <code>qs</code></p> <code>DEFAULT_CALLABLE_CHUNK_METHOD_NAME</code> <code>start_index</code> <code>int | None</code> <p><code>int</code> of <code>qs</code> start point to apply <code>method_name</code> from</p> <code>None</code> <code>end_index</code> <code>int | None</code> <p><code>int</code> of <code>qs</code> end point to apply <code>method_name</code> to</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p><code>int</code> for how many instance to batch process at a time</p> <code>DEFAULT_CALLABLE_CHUNK_SIZE</code> <code>terminal_print</code> <code>bool</code> <p>whether to print logs to terminal</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def callable_on_chunks(\n    qs: QuerySet,\n    method_name: str = DEFAULT_CALLABLE_CHUNK_METHOD_NAME,\n    start_index: int | None = None,\n    end_index: int | None = None,\n    chunk_size: int = DEFAULT_CALLABLE_CHUNK_SIZE,\n    terminal_print: bool = False,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Apply `method_name` to `qs`, filter by `start_index` and `end_index`.\n\n    Args:\n        qs: `django` `QerySet` instance to apply `method_name` total\n        method_name: `Callable` `method` of `qs` `class` to apply to `qs`\n        start_index: `int` of `qs` start point to apply `method_name` from\n        end_index: `int` of `qs` end point to apply `method_name` to\n        chunk_size: `int` for how many instance to batch process at a time\n        terminal_print: whether to print logs to terminal\n\n    Returns:\n        None\n    \"\"\"\n    model_name: str = qs.model.__name__\n    log_and_django_terminal(\n        f\"Running `{method_name}` method on `{model_name}` QuerySet.\",\n        terminal_print=terminal_print,\n    )\n\n    start_time: datetime = datetime.now()\n    qs_len: int = qs.count()\n    post_count_time: datetime = datetime.now()\n\n    count_to_process: int = (end_index or qs_len) - (start_index or 0)\n    log_and_django_terminal(\n        f\"Queryset length: {count_to_process}\", terminal_print=terminal_print\n    )\n    log_and_django_terminal(\n        f\"Queryset count time: {post_count_time - start_time}\",\n        terminal_print=terminal_print,\n    )\n    try:\n        assert count_to_process &gt;= 0\n        if start_index:\n            assert start_index &lt; qs_len\n        if end_index:\n            assert end_index &lt; qs_len\n    except AssertionError:\n        IndexError(\n            (\n                f\"Invaid indexing for model {model_name}: `start_index` {start_index}, `end_index`: {end_index}, \",\n                f\"estimated record count: {count_to_process}.\",\n            )\n        )\n\n    start_index_str: str = str(start_index or 0)\n    end_index_str: str = str(end_index or qs_len)\n    log_and_django_terminal(\n        f\"Starting to processes {count_to_process} {model_name} records at {start_time}\",\n        terminal_print=terminal_print,\n    )\n    log_and_django_terminal(\n        f\"Chunksize set to {chunk_size}\", terminal_print=terminal_print\n    )\n    log_and_django_terminal(\n        f\"Applying {method_name} from records {start_index_str} to {end_index_str} of {qs_len} of model `{model_name}`\",\n        terminal_print=terminal_print,\n    )\n    if kwargs:\n        log_and_django_terminal(\n            f\"Parameters provided for {method_name}: {kwargs}\",\n            terminal_print=terminal_print,\n        )\n\n    for record in tqdm(\n        qs[start_index:end_index].iterator(chunk_size=chunk_size),\n        total=count_to_process,\n    ):\n        getattr(record, method_name)(**kwargs)\n    end_time = datetime.now()\n    log_and_django_terminal(\n        f\"Processed records {start_index_str} to {end_index_str} (total {count_to_process}) of {qs_len} {model_name} at {end_time}.\",\n        terminal_print=terminal_print,\n    )\n    log_and_django_terminal(\n        f\"Toral process time: {end_time - post_count_time}\",\n        terminal_print=terminal_print,\n    )\n    if count_to_process:\n        log_and_django_terminal(\n            f\"Average process time: {(end_time - post_count_time)/count_to_process}\",\n            terminal_print=terminal_print,\n        )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.download_file","title":"download_file","text":"<pre><code>download_file(\n    local_path: PathLike,\n    url: str,\n    force: bool = False,\n    terminal_print: bool = True,\n) -&gt; bool\n</code></pre> <p>If <code>force</code> or not available, download <code>url</code> to <code>local_path</code>.</p> Example <pre><code>&gt;&gt;&gt; jpg_url: str = \"https://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg\"\n&gt;&gt;&gt; local_path: Path = Path('test.jpg')\n&gt;&gt;&gt; local_path.unlink(missing_ok=True)  # Ensure png deleted\n&gt;&gt;&gt; success: bool = download_file(local_path, jpg_url)\ntest.jpg not found, downloading from https://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg\nhttps://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg file available from test.jpg\n&gt;&gt;&gt; success\nTrue\n&gt;&gt;&gt; local_path.unlink()  # Delete downloaded jpg\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def download_file(\n    local_path: PathLike, url: str, force: bool = False, terminal_print: bool = True\n) -&gt; bool:\n    \"\"\"If `force` or not available, download `url` to `local_path`.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; jpg_url: str = \"https://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg\"\n        &gt;&gt;&gt; local_path: Path = Path('test.jpg')\n        &gt;&gt;&gt; local_path.unlink(missing_ok=True)  # Ensure png deleted\n        &gt;&gt;&gt; success: bool = download_file(local_path, jpg_url)\n        test.jpg not found, downloading from https://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg\n        https://commons.wikimedia.org/wiki/File:Wassily_Leontief_1973.jpg file available from test.jpg\n        &gt;&gt;&gt; success\n        True\n        &gt;&gt;&gt; local_path.unlink()  # Delete downloaded jpg\n\n        ```\n    \"\"\"\n    local_path = Path(local_path)\n    if not validate_url(url):\n        log_and_django_terminal(\n            f\"{url} is not a valid url\", terminal_print=terminal_print, level=ERROR\n        )\n        return False\n    if not local_path.exists() or force:\n        if force:\n            log_and_django_terminal(\n                f\"Overwriting {local_path} by downloading from {url}\",\n                terminal_print=terminal_print,\n            )\n        else:\n            log_and_django_terminal(\n                f\"{local_path} not found, downloading from {url}\",\n                terminal_print=terminal_print,\n            )\n        try:\n            with (\n                urlopen(url) as response,\n                open(str(local_path), \"wb\") as out_file,\n            ):\n                copyfileobj(response, out_file)\n        except IsADirectoryError:\n            log_and_django_terminal(\n                f\"{local_path} must be a file, not a directory\",\n                terminal_print=terminal_print,\n                level=ERROR,\n            )\n            return False\n        except URLError:\n            log_and_django_terminal(\n                f\"Download error (likely no internet connection): {url}\",\n                terminal_print=terminal_print,\n                level=ERROR,\n            )\n            return False\n        else:\n            log_and_django_terminal(f\"Saved to {local_path}\")\n    if not local_path.is_file():\n        log_and_django_terminal(\n            f\"{local_path} is not a file\", terminal_print=terminal_print, level=ERROR\n        )\n        return False\n    if not local_path.stat().st_size &gt; 0:\n        log_and_django_terminal(\n            f\"{local_path} from {url} is empty\",\n            terminal_print=terminal_print,\n            level=ERROR,\n        )\n        return False\n    else:\n        log_and_django_terminal(\n            f\"{url} file available from {local_path}\",\n            terminal_print=terminal_print,\n        )\n        return True\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.filter_exclude_starts_with","title":"filter_exclude_starts_with","text":"<pre><code>filter_exclude_starts_with(\n    fixture_paths: Sequence[str],\n    starts_str1: str = ITEM_MODEL_NAME,\n    starts_str2: str = NEWSPAPER_MODEL_NAME,\n    key_func: Callable = natural_keys,\n) -&gt; list[str]\n</code></pre> <p>Exclude sort <code>fixture_paths</code> starting with <code>starts_str1</code>, <code>starts_str1</code>.</p> Example <pre><code>&gt;&gt;&gt; filter_exclude_starts_with(['path/Newspaper-11.json', 'path/Issue-11.json',\n...                             'path/Item-1.json', 'cat'])\n['cat', 'path/Issue-11.json']\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def filter_exclude_starts_with(\n    fixture_paths: Sequence[str],\n    starts_str1: str = ITEM_MODEL_NAME,\n    starts_str2: str = NEWSPAPER_MODEL_NAME,\n    key_func: Callable = natural_keys,\n) -&gt; list[str]:\n    \"\"\"Exclude sort `fixture_paths` starting with `starts_str1`, `starts_str1`.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; filter_exclude_starts_with(['path/Newspaper-11.json', 'path/Issue-11.json',\n        ...                             'path/Item-1.json', 'cat'])\n        ['cat', 'path/Issue-11.json']\n\n        ```\n    \"\"\"\n    return sorted(\n        (\n            f\n            for f in fixture_paths\n            if not f.startswith(f\"{Path(f).parent}/{starts_str1}\")\n            and not f.startswith(f\"{Path(f).parent}/{starts_str2}\")\n        ),\n        key=key_func,\n    )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.filter_starts_with","title":"filter_starts_with","text":"<pre><code>filter_starts_with(\n    fixture_paths: Sequence[str],\n    starts_with_str: str = NEWSPAPER_MODEL_NAME,\n    key_func: Callable = natural_keys,\n) -&gt; list[str]\n</code></pre> <p>Filter and sort <code>fixture_paths</code> that begin with <code>starts_with_str</code>.</p> Example <pre><code>&gt;&gt;&gt; paths = ['path/Newspaper-11.json', 'path/Issue-11.json', 'path/News-1.json']\n&gt;&gt;&gt; filter_starts_with(fixture_paths=paths, starts_with_str=\"News\")\n['path/News-1.json', 'path/Newspaper-11.json']\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def filter_starts_with(\n    fixture_paths: Sequence[str],\n    starts_with_str: str = NEWSPAPER_MODEL_NAME,\n    key_func: Callable = natural_keys,\n) -&gt; list[str]:\n    \"\"\"Filter and sort `fixture_paths` that begin with `starts_with_str`.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; paths = ['path/Newspaper-11.json', 'path/Issue-11.json', 'path/News-1.json']\n        &gt;&gt;&gt; filter_starts_with(fixture_paths=paths, starts_with_str=\"News\")\n        ['path/News-1.json', 'path/Newspaper-11.json']\n\n        ```\n    \"\"\"\n    return sorted(\n        (\n            f\n            for f in fixture_paths\n            if f.startswith(f\"{Path(f).parent}/{starts_with_str}\")\n        ),\n        key=key_func,\n    )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.get_fixture_paths","title":"get_fixture_paths","text":"<pre><code>get_fixture_paths(\n    folder_path: Path | str = DEFAULT_FIXTURE_PATH,\n    format_extension: str = JSON_FORMAT_EXTENSION,\n) -&gt; list[str]\n</code></pre> <p>Return paths matching <code>folder_path</code> ending with <code>format_extension</code>.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>Path | str</code> <p>folder to search for <code>fixtures</code> in</p> <code>DEFAULT_FIXTURE_PATH</code> <code>format_extension</code> <code>str</code> <p>filename fromat extension suffix for filtering results</p> <code>JSON_FORMAT_EXTENSION</code> <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> of file path <code>str</code> with <code>format_extension</code> suffix</p> Example <pre><code>&gt;&gt;&gt; sorted(get_fixture_paths('lwmdb/tests/'))  # doctest: +NORMALIZE_WHITESPACE\n['lwmdb/tests/initial-test-dataprovider.json',\n 'lwmdb/tests/update-test-dataprovider.json']\n&gt;&gt;&gt; (\n...     get_fixture_paths('lwmdb/tests/') ==\n...     get_fixture_paths(Path('lwmdb') / 'tests')\n... )\nTrue\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def get_fixture_paths(\n    folder_path: Path | str = DEFAULT_FIXTURE_PATH,\n    format_extension: str = JSON_FORMAT_EXTENSION,\n) -&gt; list[str]:\n    \"\"\"Return paths matching `folder_path` ending with `format_extension`.\n\n    Args:\n        folder_path: folder to search for `fixtures` in\n        format_extension: filename fromat extension suffix for filtering results\n\n    Returns:\n        `list` of file path `str` with `format_extension` suffix\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; sorted(get_fixture_paths('lwmdb/tests/'))  # doctest: +NORMALIZE_WHITESPACE\n        ['lwmdb/tests/initial-test-dataprovider.json',\n         'lwmdb/tests/update-test-dataprovider.json']\n        &gt;&gt;&gt; (\n        ...     get_fixture_paths('lwmdb/tests/') ==\n        ...     get_fixture_paths(Path('lwmdb') / 'tests')\n        ... )\n        True\n\n        ```\n    \"\"\"\n    return glob(f\"{folder_path}/*{format_extension}\")\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.import_fixtures","title":"import_fixtures","text":"<pre><code>import_fixtures(\n    ordered_fixture_paths: list[str],\n    start_index: int | None = None,\n    end_index: int | None = None,\n    django_command_instance: BaseCommand | None = None,\n) -&gt; None\n</code></pre> <p>Call <code>loaddata</code> on fixtuers in <code>ordered_fixture_paths</code>.</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def import_fixtures(\n    ordered_fixture_paths: list[str],\n    start_index: int | None = None,\n    end_index: int | None = None,\n    django_command_instance: BaseCommand | None = None,\n) -&gt; None:\n    \"\"\"Call `loaddata` on fixtuers in `ordered_fixture_paths`.\"\"\"\n    success_style = (\n        django_command_instance.style.SUCCESS if django_command_instance else None\n    )\n    tstart = datetime.now()\n    log_and_django_terminal(\n        f\"Starting: {tstart}\",\n        django_command_instance=django_command_instance,\n        style=success_style,\n    )\n    fixture_paths: list[str] = ordered_fixture_paths[start_index:end_index]\n    for i, path in enumerate(fixture_paths):\n        t1 = datetime.now()\n        log_and_django_terminal(\n            f\"Starting import {i+1} of {len(fixture_paths)}: {t1}\",\n            django_command_instance=django_command_instance,\n            style=success_style,\n        )\n        call_command(\"loaddata\", path, verbosity=3)\n        t2 = datetime.now()\n        log_and_django_terminal(\n            f\"Import of path {path} took: {t2 - t1}\",\n            django_command_instance=django_command_instance,\n            style=success_style,\n        )\n        log_and_django_terminal(\n            f\"Import time thus far: {t2 - tstart}\",\n            django_command_instance=django_command_instance,\n            style=success_style,\n        )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.log_and_django_terminal","title":"log_and_django_terminal","text":"<pre><code>log_and_django_terminal(\n    message: str,\n    terminal_print: bool = False,\n    level: int = INFO,\n    django_command_instance: BaseCommand | None = None,\n    style: Callable | None = None,\n    *arg,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Log and add Django formatted print to terminal if available.</p> Note <p>See: https://code.djangoproject.com/ticket/21429</p> <p>Parameters:</p> Name Type Description Default <code>messsage</code> <p><code>str</code> to log and potential print to terminal</p> required <code>terminal_print</code> <code>bool</code> <p>whether to print  to terminal as well</p> <code>False</code> <code>level</code> <code>int</code> <p>what <code>logger</code> level to create</p> <code>INFO</code> <code>django_command_instance</code> <code>BaseCommand | None</code> <p><code>BaseCommand</code> or subclass instance to manage <code>terminal interaction</code></p> <code>None</code> <code>style</code> <code>Callable | None</code> <p>function to call on <code>message</code> prior to sending to <code>django_command_instance</code> if provided. No effect without <code>django_command_instance</code></p> <code>None</code> <code>args</code> <p>any positional arguments to send to <code>logger.log</code> call</p> required <code>kwargs</code> <p>keyword arguments  to pass to <code>logger.log</code> call</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def log_and_django_terminal(\n    message: str,\n    terminal_print: bool = False,\n    level: int = INFO,\n    django_command_instance: BaseCommand | None = None,\n    style: Callable | None = None,\n    *arg,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Log and add Django formatted print to terminal if available.\n\n    Note:\n        See: https://code.djangoproject.com/ticket/21429\n\n    Args:\n        messsage: `str` to log and potential print to terminal\n        terminal_print: whether to print  to terminal as well\n        level: what `logger` level to create\n        django_command_instance:\n            `BaseCommand` or subclass instance to manage `terminal interaction`\n        style:\n            function to call on `message` prior to sending to\n            `django_command_instance` if provided. No effect\n            without `django_command_instance`\n        args: any positional arguments to send to `logger.log` call\n        kwargs: keyword arguments  to pass to `logger.log` call\n\n    Returns:\n        None\n    \"\"\"\n    logger.log(level, message, *arg, **kwargs)\n    if terminal_print:\n        print(message)\n    if django_command_instance:\n        if style:\n            django_command_instance.stdout.write(style(message))\n        else:\n            django_command_instance.stdout.write(message)\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.natural_keys","title":"natural_keys","text":"<pre><code>natural_keys(\n    text: str,\n    split_regex: str = DIGITS_REGEX,\n    func: Callable[[str], int | str] = text2int,\n) -&gt; list[str | int]\n</code></pre> <p>Split <code>text</code> to strs and/or numbers if possible via <code>func</code>.</p> <p>Designed for application to a list of fixture filenames with integers</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p><code>str</code> instance to process as a key</p> required <code>split_regex</code> <code>str</code> <p>a regular expression to split keys, default extracts digits</p> <code>DIGITS_REGEX</code> <code>func</code> <code>Callable[[str], int | str]</code> <p>function to call on the results of <code>split_regex</code>, the results are can be used with <code>sorted</code> for ordering.</p> <code>text2int</code> Example <pre><code>&gt;&gt;&gt; example = [\"fixture/Item-1.json\", \"fixture/Item-10.json\", \"fixture/Item-2.json\"]\n&gt;&gt;&gt; sorted(example, key=natural_keys)\n['fixture/Item-1.json', 'fixture/Item-2.json', 'fixture/Item-10.json']\n</code></pre> Inspiration <p>http://nedbatchelder.com/blog/200712/human_sorting.html</p> Source code in <code>lwmdb/utils.py</code> <pre><code>def natural_keys(\n    text: str,\n    split_regex: str = DIGITS_REGEX,\n    func: Callable[[str], int | str] = text2int,\n) -&gt; list[str | int]:\n    \"\"\"Split `text` to strs and/or numbers if possible via `func`.\n\n    Designed for application to a list of fixture filenames with integers\n\n    Args:\n        text: `str` instance to process as a key\n        split_regex: a regular expression to split keys, default extracts digits\n        func:\n            function to call on the results of `split_regex`, the results are\n            can be used with `sorted` for ordering.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; example = [\"fixture/Item-1.json\", \"fixture/Item-10.json\", \"fixture/Item-2.json\"]\n        &gt;&gt;&gt; sorted(example, key=natural_keys)\n        ['fixture/Item-1.json', 'fixture/Item-2.json', 'fixture/Item-10.json']\n\n        ```\n\n    Inspiration:\n        http://nedbatchelder.com/blog/200712/human_sorting.html\n    \"\"\"\n    return [func(c) for c in re.split(split_regex, text)]\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.path_or_str_suffix","title":"path_or_str_suffix","text":"<pre><code>path_or_str_suffix(\n    str_or_path: str | PathLike,\n    max_extension_len: int = 10,\n    force: bool = False,\n    split_str: str = \".\",\n) -&gt; str\n</code></pre> <p>Return suffix of <code>str_or_path</code>, else \u2018\u2019.</p> Example <pre><code>&gt;&gt;&gt; path_or_str_suffix('https://lwmd.livingwithmachines.ac.uk/file.bz2')\n'bz2'\n\n&gt;&gt;&gt; path_or_str_suffix('https://lwmd.livingwithmachines.ac.uk/file')\n''\n\n&gt;&gt;&gt; path_or_str_suffix(Path('cat') / 'dog' / 'fish.csv')\n'csv'\n&gt;&gt;&gt; path_or_str_suffix(Path('cat') / 'dog' / 'fish')\n''\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def path_or_str_suffix(\n    str_or_path: str | PathLike,\n    max_extension_len: int = 10,\n    force: bool = False,\n    split_str: str = \".\",\n) -&gt; str:\n    \"\"\"Return suffix of `str_or_path`, else ''.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; path_or_str_suffix('https://lwmd.livingwithmachines.ac.uk/file.bz2')\n        'bz2'\n\n        &gt;&gt;&gt; path_or_str_suffix('https://lwmd.livingwithmachines.ac.uk/file')\n        ''\n\n        &gt;&gt;&gt; path_or_str_suffix(Path('cat') / 'dog' / 'fish.csv')\n        'csv'\n        &gt;&gt;&gt; path_or_str_suffix(Path('cat') / 'dog' / 'fish')\n        ''\n\n        ```\n    \"\"\"\n    suffix: str = \"\"\n    if isinstance(str_or_path, Path):\n        if str_or_path.suffix:\n            suffix = str_or_path.suffix[1:]  # Skip the `.` for consistency\n        else:\n            \"\"\"\"\"\"\n    else:\n        split_str_or_path: list[str] = str(str_or_path).split(split_str)\n        if len(split_str_or_path) &gt; 1:\n            suffix = split_str_or_path[-1]\n            if \"/\" in suffix:\n                log_and_django_terminal(\n                    f\"Split via {split_str} of \"\n                    f\"{str_or_path} has a `/` `char`. \"\n                    \"Returning ''\",\n                    level=ERROR,\n                )\n                return \"\"\n        else:\n            log_and_django_terminal(\n                f\"Can't split via {split_str} in \"\n                f\"{_short_text_trunc(str(str_or_path))}\",\n                level=ERROR,\n            )\n            return \"\"\n    if len(suffix) &gt; max_extension_len:\n        if force:\n            log_and_django_terminal(f\"Force return of suffix {suffix}\", level=WARNING)\n            return suffix\n        else:\n            log_and_django_terminal(\n                f\"suffix {_short_text_trunc(suffix)} too long \"\n                f\"(max={max_extension_len})\",\n                level=ERROR,\n            )\n            return \"\"\n    else:\n        return suffix\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.sort_all_fixture_paths","title":"sort_all_fixture_paths","text":"<pre><code>sort_all_fixture_paths(\n    unsorted_fixture_paths: Sequence[str], key_func: Callable = natural_keys\n) -&gt; list[str]\n</code></pre> <p>Sort fixture paths for <code>Newspaper.models</code> with order compatibility.</p> <p>Certain models within the <code>newspapers</code> <code>app</code> require a specific order to correctly import fixtures. This is written for potential generic application but primarily ment for <code>fixtures</code> including <code>newspapers</code> tables.</p> <p>Parameters:</p> Name Type Description Default <code>unsorted_fixture_paths</code> <code>Sequence[str]</code> <p><code>Sequence</code> of <code>str</code> <code>fixture</code> paths to sort</p> required <code>key_fun</code> <p>function to call to order <code>unsorted_fixture_paths</code></p> required <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> of sorted paths <code>str</code> via <code>key_func</code></p> Example <pre><code>&gt;&gt;&gt; paths = [\n...     'path/Item-1.json', 'path/Newspaper-11.json',\n...     'path/Issue-11.json', 'cat'\n... ]\n&gt;&gt;&gt; sort_all_fixture_paths(paths)\n['path/Newspaper-11.json', 'cat', 'path/Issue-11.json', 'path/Item-1.json']\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def sort_all_fixture_paths(\n    unsorted_fixture_paths: Sequence[str], key_func: Callable = natural_keys\n) -&gt; list[str]:\n    \"\"\"Sort fixture paths for `Newspaper.models` with order compatibility.\n\n    Certain models within the `newspapers` `app` require a specific order to correctly\n    import fixtures. This is written for potential generic application but primarily ment\n    for `fixtures` including `newspapers` tables.\n\n    Args:\n        unsorted_fixture_paths: `Sequence` of `str` `fixture` paths to sort\n        key_fun: function to call to order `unsorted_fixture_paths`\n\n    Returns:\n        `list` of sorted paths `str` via `key_func`\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; paths = [\n        ...     'path/Item-1.json', 'path/Newspaper-11.json',\n        ...     'path/Issue-11.json', 'cat'\n        ... ]\n        &gt;&gt;&gt; sort_all_fixture_paths(paths)\n        ['path/Newspaper-11.json', 'cat', 'path/Issue-11.json', 'path/Item-1.json']\n\n        ```\n    \"\"\"\n    newspaper_fixture_paths: list[str] = filter_starts_with(\n        unsorted_fixture_paths, NEWSPAPER_MODEL_NAME, key_func\n    )\n    non_newspaper_non_item_fixture_paths: list[str] = filter_exclude_starts_with(\n        fixture_paths=unsorted_fixture_paths,\n        key_func=key_func,\n    )\n    item_fixtures: list[str] = filter_starts_with(\n        unsorted_fixture_paths, ITEM_MODEL_NAME, key_func\n    )\n    return (\n        newspaper_fixture_paths + non_newspaper_non_item_fixture_paths + item_fixtures\n    )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.str_to_bool","title":"str_to_bool","text":"<pre><code>str_to_bool(\n    val: str,\n    true_strs: Sequence[str] = VALID_TRUE_STRS,\n    false_strs: Sequence[str] = VALID_FALSE_STRS,\n) -&gt; bool\n</code></pre> <p>Convert values of <code>true_strs</code> to <code>True</code> and <code>false_strs</code> to <code>False</code>.</p> Note <p>See https://docs.python.org/3/distutils/apiref.html#distutils.util.strtobool which is due to depricate, hence equivalent below.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <p><code>str</code> to convert into a <code>bool</code></p> required <code>true_strs</code> <code>Sequence[str]</code> <p>a <code>Sequence</code> of <code>str</code> values treated as <code>True</code></p> <code>VALID_TRUE_STRS</code> <code>false_strs</code> <code>Sequence[str]</code> <p>a <code>Sequence</code> of <code>str</code> values treated as <code>False</code></p> <code>VALID_FALSE_STRS</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>val</code> in <code>true_strs</code>, <code>False</code> if <code>val</code> in <code>false_str</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>val</code>, lowercased, is not a <code>str</code> in the <code>true_strs</code></p> Example <pre><code>&gt;&gt;&gt; str_to_bool('true')\nTrue\n&gt;&gt;&gt; str_to_bool('FaLSe')\nFalse\n&gt;&gt;&gt; str_to_bool('Truely')\nTraceback (most recent call last):\n...\nValueError: `Truely` dose not match `True` values:\n('y', 'yes', 't', 'true', 'on', '1')\nor `False` values:\n('n', 'no', 'f', 'false', 'off', '0')\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def str_to_bool(\n    val: str,\n    true_strs: Sequence[str] = VALID_TRUE_STRS,\n    false_strs: Sequence[str] = VALID_FALSE_STRS,\n) -&gt; bool:\n    \"\"\"Convert values of `true_strs` to `True` and `false_strs` to `False`.\n\n    Note:\n        See\n        https://docs.python.org/3/distutils/apiref.html#distutils.util.strtobool\n        which is due to depricate, hence equivalent below.\n\n    Args:\n        var: `str` to convert into a `bool`\n        true_strs: a `Sequence` of `str` values treated as `True`\n        false_strs: a `Sequence` of `str` values treated as `False`\n\n    Returns:\n        `True` if `val` in `true_strs`, `False` if `val` in `false_str`.\n\n    Raises:\n        ValueError: if `val`, lowercased, is not a `str` in the `true_strs`\n        or `false_strs`.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; str_to_bool('true')\n        True\n        &gt;&gt;&gt; str_to_bool('FaLSe')\n        False\n        &gt;&gt;&gt; str_to_bool('Truely')\n        Traceback (most recent call last):\n        ...\n        ValueError: `Truely` dose not match `True` values:\n        ('y', 'yes', 't', 'true', 'on', '1')\n        or `False` values:\n        ('n', 'no', 'f', 'false', 'off', '0')\n\n        ```\n    \"\"\"\n    if val.lower() in true_strs:\n        return True\n    elif val.lower() in false_strs:\n        return False\n    else:\n        raise ValueError(\n            f\"`{val}` dose not match `True` values:\"\n            f\"\\n{true_strs}\\n\"\n            f\"or `False` values:\"\n            f\"\\n{false_strs}\"\n        )\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.text2int","title":"text2int","text":"<pre><code>text2int(text: str) -&gt; int | str\n</code></pre> <p>If <code>text</code> is a sequence of digits convert <code>text</code> to <code>int</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p><code>str</code> to convert to <code>int</code> if possible</p> required <p>Returns:</p> Type Description <code>int | str</code> <p><code>int</code> if <code>text</code> is a number, else the original <code>text</code></p> Example <pre><code>&gt;&gt;&gt; text2int('cat')\n'cat'\n&gt;&gt;&gt; text2int('10')\n10\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def text2int(text: str) -&gt; int | str:\n    \"\"\"If `text` is a sequence of digits convert `text` to `int`.\n\n    Args:\n        text: `str` to convert to `int` if possible\n\n    Returns:\n        `int` if `text` is a number, else the original `text`\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; text2int('cat')\n        'cat'\n        &gt;&gt;&gt; text2int('10')\n        10\n\n        ```\n    \"\"\"\n    return int(text) if text.isdigit() else text\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.truncate_str","title":"truncate_str","text":"<pre><code>truncate_str(\n    text: str,\n    max_length: int = DEFAULT_MAX_LOG_STR_LENGTH,\n    trail_str: str = DEFAULT_TRUNCATION_CHARS,\n) -&gt; str\n</code></pre> <p>If <code>len(text) &gt; max_length</code> return <code>text</code> followed by <code>trail_str</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p><code>str</code> to truncate</p> required <code>max_length</code> <code>int</code> <p>maximum length of <code>text</code> to allow, anything belond truncated</p> <code>DEFAULT_MAX_LOG_STR_LENGTH</code> <code>trail_str</code> <code>str</code> <p>what is appended to the end of <code>text</code> if truncated</p> <code>DEFAULT_TRUNCATION_CHARS</code> <p>Returns:</p> Type Description <code>str</code> <p><code>text</code> truncated to <code>max_length</code> (if longer than <code>max_length</code>),</p> <code>str</code> <p>appended with <code>tail_str</code></p> Example <pre><code>&gt;&gt;&gt; truncate_str('Standing in the shadows of love.', 15)\n'Standing in the...'\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def truncate_str(\n    text: str,\n    max_length: int = DEFAULT_MAX_LOG_STR_LENGTH,\n    trail_str: str = DEFAULT_TRUNCATION_CHARS,\n) -&gt; str:\n    \"\"\"If `len(text) &gt; max_length` return `text` followed by `trail_str`.\n\n    Args:\n        text: `str` to truncate\n        max_length: maximum length of `text` to allow, anything belond truncated\n        trail_str: what is appended to the end of `text` if truncated\n\n    Returns:\n        `text` truncated to `max_length` (if longer than `max_length`),\n        appended with `tail_str`\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; truncate_str('Standing in the shadows of love.', 15)\n        'Standing in the...'\n\n        ```\n    \"\"\"\n    return text[:max_length] + trail_str if len(text) &gt; max_length else text\n</code></pre>"},{"location":"reference/lwmdb/utils/#lwmdb.utils.word_count","title":"word_count","text":"<pre><code>word_count(text: str) -&gt; int\n</code></pre> <p>Assuming English sentence structure, count words in <code>text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to count words from</p> required <p>Returns:</p> Type Description <code>int</code> <p>Count of words <code>text</code>.</p> Example <pre><code>&gt;&gt;&gt; word_count(\"A big brown dog, left-leaning, loomed! Another joined.\")\n8\n</code></pre> Source code in <code>lwmdb/utils.py</code> <pre><code>def word_count(text: str) -&gt; int:\n    \"\"\"Assuming English sentence structure, count words in `text`.\n\n    Args:\n        text: text to count words from\n\n    Returns:\n        Count of words `text`.\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; word_count(\"A big brown dog, left-leaning, loomed! Another joined.\")\n        8\n\n        ```\n    \"\"\"\n    return len(text.split())\n</code></pre>"},{"location":"reference/lwmdb/contrib/","title":"contrib","text":"<p>To understand why this file is here, see the link below.</p> <ul> <li>http://cookiecutter-django.readthedocs.io/en/latest/faq.html#why-is- there-a-django-contrib-sites-directory-in-cookiecutter-django</li> </ul>"},{"location":"reference/lwmdb/contrib/sites/","title":"sites","text":"<p>To understand why this file is here, see the link below.</p> <ul> <li>http://cookiecutter-django.readthedocs.io/en/latest/faq.html#why-is- there-a-django-contrib-sites-directory-in-cookiecutter-django</li> </ul>"},{"location":"reference/lwmdb/contrib/sites/migrations/","title":"migrations","text":"<p>To understand why this file is here, please read:</p> <p>http://cookiecutter-django.readthedocs.io/en/latest/faq.html#why-is-there-a-django-contrib-sites-directory-in-cookiecutter-django</p>"},{"location":"reference/lwmdb/management/","title":"management","text":""},{"location":"reference/lwmdb/management/commands/","title":"commands","text":""},{"location":"reference/lwmdb/management/commands/connect/","title":"connect","text":""},{"location":"reference/lwmdb/management/commands/createfixtures/","title":"createfixtures","text":""},{"location":"reference/lwmdb/management/commands/fixtures/","title":"fixtures","text":""},{"location":"reference/lwmdb/management/commands/load_json_fixtures/","title":"load_json_fixtures","text":""},{"location":"reference/lwmdb/management/commands/load_json_fixtures/#lwmdb.management.commands.load_json_fixtures.Command","title":"Command","text":"<p>             Bases: <code>BaseCommand</code></p> <p>Load a collection of <code>json</code> data in the correct order.</p> Source code in <code>lwmdb/management/commands/load_json_fixtures.py</code> <pre><code>class Command(BaseCommand):\n    \"\"\"Load a collection of `json` data in the correct order.\"\"\"\n\n    help: str = \"Loads all `json` files in the `fixtures` path in an enforced order\"\n    path: Path = Path(DEFAULT_FIXTURE_PATH)\n    format_extension: str = JSON_FORMAT_EXTENSION\n\n    def add_arguments(self, parser) -&gt; None:\n        parser.add_argument(\n            \"--path\", nargs=\"?\", const=1, type=str, default=str(self.path)\n        )\n        parser.add_argument(\"--start-index\", nargs=\"?\", const=1, type=int)\n        parser.add_argument(\"--end-index\", nargs=\"?\", const=1, type=int)\n\n    def handle(self, *args, **options) -&gt; None:\n        if options[\"path\"]:\n            self.path = Path(options[\"path\"])\n        start_index: int | None = (\n            options[\"start_index\"] if options[\"start_index\"] else None\n        )\n        end_index: int | None = options[\"end_index\"] if options[\"end_index\"] else None\n        self.stdout.write(self.style.SUCCESS(f\"Loading fixtures from {self.path}\"))\n        self._unsorted_fixture_paths: list[str] = get_fixture_paths(\n            self.path, self.format_extension\n        )\n        self.fixture_paths = sort_all_fixture_paths(self._unsorted_fixture_paths)\n        import_fixtures(\n            self.fixture_paths,\n            start_index=start_index,\n            end_index=end_index,\n            django_command_instance=self,\n        )\n</code></pre>"},{"location":"reference/lwmdb/management/commands/loadfixtures/","title":"loadfixtures","text":""},{"location":"reference/lwmdb/management/commands/makeitemfixtures/","title":"makeitemfixtures","text":""},{"location":"reference/lwmdb/management/commands/makeitemfixtures/#lwmdb.management.commands.makeitemfixtures.fix_fields","title":"fix_fields","text":"<pre><code>fix_fields(fields)\n</code></pre> <p>Check correctness of newspapers Item model fields.</p> <p>Iterate over the the <code>digitisation</code>, <code>ingest</code>, <code>data_provider</code>, <code>issue</code>, <code>ocr_quality_mean</code> and <code>ocr_quality_sd</code> fields, adjusting if necessarry. Conclude by updating the <code>created_at</code> and <code>updated_at</code> fields.</p> Source code in <code>lwmdb/management/commands/makeitemfixtures.py</code> <pre><code>def fix_fields(fields):\n    \"\"\"Check correctness of newspapers Item model fields.\n\n    Iterate over the the `digitisation`, `ingest`, `data_provider`,\n    `issue`, `ocr_quality_mean` and `ocr_quality_sd` fields, adjusting\n    if necessarry. Conclude by updating the `created_at` and\n    `updated_at` fields.\n    \"\"\"\n    locator_software = fields.get(\"digitisation__software\")\n    if not LOCAL_STORE[\"software\"].get(locator_software):\n        LOCAL_STORE[\"software\"][locator_software] = Digitisation.objects.get(\n            software=locator_software\n        ).pk\n    del fields[\"digitisation__software\"]\n\n    locator_ingest = (\n        fields.get(\"ingest__lwm_tool_name\")\n        + \"-\"\n        + fields.get(\"ingest__lwm_tool_version\")\n    )\n    if not LOCAL_STORE[\"ingest\"].get(locator_ingest):\n        LOCAL_STORE[\"ingest\"][locator_ingest] = Ingest.objects.get(\n            lwm_tool_name=fields.get(\"ingest__lwm_tool_name\"),\n            lwm_tool_version=fields.get(\"ingest__lwm_tool_version\"),\n        ).pk\n    del fields[\"ingest__lwm_tool_name\"]\n    del fields[\"ingest__lwm_tool_version\"]\n\n    locator_dp = fields.get(\"data_provider\")\n    if not LOCAL_STORE[\"data_provider\"].get(locator_dp):\n        LOCAL_STORE[\"data_provider\"][locator_dp] = DataProvider.objects.get(\n            name=fields.get(\"data_provider\")\n        ).pk\n    del fields[\"data_provider\"]\n\n    locator_issue = fields.get(\"issue__issue_identifier\")\n    if not LOCAL_STORE[\"issue\"].get(locator_issue):\n        LOCAL_STORE[\"issue\"][locator_issue] = Issue.objects.get(\n            issue_code=fields.get(\"issue__issue_identifier\")\n        ).pk\n    del fields[\"issue__issue_identifier\"]\n\n    fields[\"digitisation\"] = LOCAL_STORE[\"software\"][locator_software]\n    fields[\"ingest\"] = LOCAL_STORE[\"ingest\"][locator_ingest]\n    fields[\"data_provider\"] = LOCAL_STORE[\"data_provider\"][locator_dp]\n    fields[\"issue\"] = LOCAL_STORE[\"issue\"][locator_issue]\n\n    if not fields[\"ocr_quality_mean\"] or fields[\"ocr_quality_mean\"] == \"\":\n        fields[\"ocr_quality_mean\"] = 0\n\n    if not fields[\"ocr_quality_sd\"] or fields[\"ocr_quality_sd\"] == \"\":\n        fields[\"ocr_quality_sd\"] = 0\n\n    fields[\"created_at\"] = str(current)\n    fields[\"updated_at\"] = str(current)\n\n    return fields\n</code></pre>"},{"location":"reference/lwmdb/tests/","title":"tests","text":""},{"location":"reference/mitchells/","title":"mitchells","text":""},{"location":"reference/mitchells/admin/","title":"admin","text":""},{"location":"reference/mitchells/apps/","title":"apps","text":""},{"location":"reference/mitchells/import_fixtures/","title":"import_fixtures","text":""},{"location":"reference/mitchells/models/","title":"models","text":""},{"location":"reference/mitchells/views/","title":"views","text":""},{"location":"reference/mitchells/migrations/","title":"migrations","text":""},{"location":"reference/newspapers/","title":"newspapers","text":""},{"location":"reference/newspapers/admin/","title":"admin","text":""},{"location":"reference/newspapers/apps/","title":"apps","text":""},{"location":"reference/newspapers/models/","title":"models","text":""},{"location":"reference/newspapers/models/#newspapers.models.DataProvider","title":"DataProvider","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Source and collection information for data provider.</p> Source code in <code>newspapers/models.py</code> <pre><code>class DataProvider(NewspapersModel):\n    \"\"\"Source and collection information for data provider.\"\"\"\n\n    name = models.CharField(max_length=600, default=None)\n    collection = models.CharField(max_length=600, default=None)\n    source_note = models.CharField(max_length=255, default=None)\n    code = models.SlugField(\n        max_length=100, default=None, null=True, blank=True, unique=True\n    )\n    legacy_code = models.SlugField(\n        max_length=100, default=None, null=True, blank=True, unique=True\n    )\n\n    class Meta:\n        unique_together = [(\"name\", \"collection\")]\n\n    def __str__(self):\n        return truncate_str(self.name, max_length=MAX_PRINT_SELF_STR_LENGTH)\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Digitisation","title":"Digitisation","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Means by which collection was digitised.</p> Source code in <code>newspapers/models.py</code> <pre><code>class Digitisation(NewspapersModel):\n    \"\"\"Means by which collection was digitised.\"\"\"\n\n    xml_flavour = models.CharField(max_length=255, default=None)\n    software = models.CharField(max_length=600, default=None, null=True, blank=True)\n    mets_namespace = models.CharField(\n        max_length=255, default=None, blank=True, null=True\n    )\n    alto_namespace = models.CharField(\n        max_length=255, default=None, blank=True, null=True\n    )\n\n    class Meta:\n        unique_together = (\"xml_flavour\", \"software\")\n\n    def __str__(self):\n        return f\"{self.mets_namespace}, {self.alto_namespace}\"\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Ingest","title":"Ingest","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Tool used for database ingest.</p> Source code in <code>newspapers/models.py</code> <pre><code>class Ingest(NewspapersModel):\n    \"\"\"Tool used for database ingest.\"\"\"\n\n    lwm_tool_name = models.CharField(max_length=600, default=None)\n    lwm_tool_version = models.CharField(max_length=600, default=None)\n    lwm_tool_source = models.CharField(max_length=255, default=None)\n\n    class Meta:\n        unique_together = (\"lwm_tool_name\", \"lwm_tool_version\")\n\n    def __str__(self):\n        return truncate_str(self.lwm_tool_name, max_length=MAX_PRINT_SELF_STR_LENGTH)\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Issue","title":"Issue","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Newspaper Issue, including date and relevant source url.</p> Source code in <code>newspapers/models.py</code> <pre><code>class Issue(NewspapersModel):\n    \"\"\"Newspaper Issue, including date and relevant source url.\"\"\"\n\n    # TODO #55: issue_code should be unique? Currently unique (but not tested with BNA)\n    issue_code = models.CharField(max_length=600, default=None)\n    issue_date = models.DateField()\n    input_sub_path = models.CharField(max_length=255, default=None)\n\n    newspaper = models.ForeignKey(\n        Newspaper,\n        on_delete=models.SET_NULL,\n        verbose_name=\"newspaper\",\n        null=True,\n        related_name=\"issues\",\n        related_query_name=\"issue\",\n    )\n\n    def __str__(self):\n        return str(self.issue_code)\n\n    @property\n    def url(self):\n        \"\"\"Return a URL similar to the British Newspaper Archive structure.\n\n        Example:\n            https://www.britishnewspaperarchive.co.uk/viewer/BL/0000347/18890102/001/0001\n        \"\"\"\n        if not self.issue_date:\n            print(\"Warning: No date available for issue so URL will likely not work.\")\n\n        return (\n            f\"https://www.britishnewspaperarchive.co.uk/viewer/BL/\"\n            f\"{self.newspaper.publication_code}/\"\n            f\"{str(self.issue_date).replace('-', '')}/001/0001\"\n        )\n\n    class Meta:\n        indexes = [\n            models.Index(\n                fields=[\n                    \"issue_code\",\n                ]\n            )\n        ]\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Issue.url","title":"url  <code>property</code>","text":"<pre><code>url\n</code></pre> <p>Return a URL similar to the British Newspaper Archive structure.</p> Example <p>https://www.britishnewspaperarchive.co.uk/viewer/BL/0000347/18890102/001/0001</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item","title":"Item","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Printed element in a Newspaper issue including metadata.</p> Source code in <code>newspapers/models.py</code> <pre><code>class Item(NewspapersModel):\n    \"\"\"Printed element in a Newspaper issue including metadata.\"\"\"\n\n    # TODO #55: item_code should be unique? Currently, not unique, however so needs fixing in alto2txt2fixture\n    MAX_TITLE_CHAR_COUNT: Final[int] = 100\n\n    item_code = models.CharField(max_length=600, default=None)\n    title = models.TextField(max_length=MAX_TITLE_CHAR_COUNT, default=None)\n    title_word_count = models.IntegerField(null=True)\n    title_char_count = models.IntegerField(null=True)\n    title_truncated = models.BooleanField(default=False)\n    item_type = models.CharField(max_length=600, default=None, blank=True, null=True)\n    word_count = models.IntegerField(null=True, db_index=True)\n    word_char_count = models.IntegerField(null=True)\n    ocr_quality_mean = models.FloatField(null=True, blank=True)\n    ocr_quality_sd = models.FloatField(null=True, blank=True)\n    input_filename = models.CharField(max_length=255, default=None)\n    issue = models.ForeignKey(\n        Issue,\n        on_delete=models.SET_NULL,\n        verbose_name=\"issue\",\n        null=True,\n        related_name=\"items\",\n        related_query_name=\"item\",\n    )\n    data_provider = models.ForeignKey(\n        DataProvider,\n        on_delete=models.SET_NULL,\n        verbose_name=\"data_provider\",\n        null=True,\n        related_name=\"items\",\n        related_query_name=\"item\",\n    )\n    digitisation = models.ForeignKey(\n        Digitisation,\n        on_delete=models.SET_NULL,\n        verbose_name=\"digitisation\",\n        null=True,\n        related_name=\"items\",\n        related_query_name=\"item\",\n    )\n    ingest = models.ForeignKey(\n        Ingest,\n        on_delete=models.SET_NULL,\n        verbose_name=\"ingest\",\n        null=True,\n        related_name=\"items\",\n        related_query_name=\"item\",\n    )\n    fulltext = models.OneToOneField(Fulltext, null=True, on_delete=models.SET_NULL)\n\n    class Meta:\n        indexes = [\n            models.Index(\n                fields=[\n                    \"item_code\",\n                ]\n            )\n        ]\n\n    def save(self, sync_title_counts: bool = False, *args, **kwargs):\n        # for consistency, we save all item_type in uppercase\n        self.item_type = str(self.item_type).upper()\n        self._sync_title_counts(force=sync_title_counts)\n        return super().save(*args, **kwargs)\n\n    def __str__(self):\n        return truncate_str(self.title, max_length=MAX_PRINT_SELF_STR_LENGTH)\n\n    def __repr__(self):\n        return str(self.item_code)\n\n    def _sync_title_char_count(self, force: bool = False) -&gt; None:\n        title_text_char_count: int = len(self.title)\n        if not self.title_char_count or force:\n            logger.debug(\n                f\"Setting `title_char_count` for {self.item_code} to {title_text_char_count}\"\n            )\n            self.title_char_count = title_text_char_count\n        else:\n            if self.title_char_count != title_text_char_count:\n                if self.title_char_count &lt; self.MAX_TITLE_CHAR_COUNT:\n                    raise self.TitleLengthError(\n                        f\"{self.title_char_count} characters does not equal {title_text_char_count}: the length of title of {self.item_code}\"\n                    )\n\n    def _sync_title_word_count(self, force: bool = False) -&gt; None:\n        title_text_word_count: int = word_count(self.title)\n        if not self.title_word_count or force:\n            logger.debug(\n                f\"Setting `title_word_count` for {self.item_code} to {title_text_word_count}\"\n            )\n            self.title_word_count = title_text_word_count\n        else:\n            if self.title_word_count != title_text_word_count:\n                raise self.TitleLengthError(\n                    f\"{self.title_word_count} does not equal {title_text_word_count}: the length of words in title of {self}\"\n                )\n\n    def _sync_title_counts(self, force: bool = False) -&gt; None:\n        \"\"\"Run `_sync` methods, then trim title if long.\"\"\"\n        self._sync_title_char_count(force=force)\n        self._sync_title_word_count(force=force)\n        if self.title_char_count and self.title_char_count &gt; self.MAX_TITLE_CHAR_COUNT:\n            logger.debug(\n                f\"Trimming title of {self.item_code} to {self.MAX_TITLE_CHAR_COUNT} chars.\"\n            )\n            self.title = self.title[: self.MAX_TITLE_CHAR_COUNT]\n            self.title_truncated = True\n\n    HOME_DIR = Path.home()\n    DOWNLOAD_DIR = HOME_DIR / \"metadata-db/\"\n    ARCHIVE_SUBDIR = \"archives\"\n    EXTRACTED_SUBDIR = \"articles\"\n    FULLTEXT_METHOD = \"download\"\n    FULLTEXT_CONTAINER_SUFFIX = \"-alto2txt\"\n    FULLTEXT_CONTAINER_PATH = \"plaintext/\"\n    FULLTEXT_STORAGE_ACCOUNT_URL = \"https://alto2txt.blob.core.windows.net\"\n\n    SAS_ENV_VARIABLE = \"FULLTEXT_SAS_TOKEN\"\n\n    class TitleLengthError(Exception):\n        ...\n\n    @property\n    def download_dir(self):\n        \"\"\"Path to the download directory for full text data.\n\n        The DOWNLOAD_DIR class attribute contains the directory under\n        which full text data will be stored. Users can change it by\n        typing: Item.DOWNLOAD_DIR = \"/path/to/wherever/\"\n        \"\"\"\n        return Path(self.DOWNLOAD_DIR)\n\n    @property\n    def text_archive_dir(self):\n        \"\"\"Path to the storage directory for full text archives.\"\"\"\n        return self.download_dir / self.ARCHIVE_SUBDIR\n\n    @property\n    def text_extracted_dir(self):\n        \"\"\"Path to the storage directory for extracted full text files.\"\"\"\n        return self.download_dir / self.EXTRACTED_SUBDIR\n\n    @property\n    def zip_file(self):\n        \"\"\"Filename for this Item's zip archive containing the full text.\"\"\"\n        return f\"{self.issue.newspaper.publication_code}_plaintext.zip\"\n\n    @property\n    def text_container(self):\n        \"\"\"Azure blob storage container containing the Item full text.\"\"\"\n        return f\"{self.data_provider.name}{self.FULLTEXT_CONTAINER_SUFFIX}\"\n\n    @property\n    def text_path(self):\n        \"\"\"Return a path relative to the full text file for this Item.\n\n        This is generated from the zip archive (once downloaded and\n        extracted) from the DOWNLOAD_DIR and the filename.\n        \"\"\"\n        return Path(self.issue.input_sub_path) / self.input_filename\n\n    # Commenting this out as it will fail with the dev on #56 (see branch kallewesterling/issue56).\n    # As this will likely not be the first go-to for fulltext access, we can keep it as a method:\n    # .extract_fulltext()\n    #\n    # @property\n    # def fulltext(self):\n    #     try:\n    #         return self.extract_fulltext()\n    #     except Exception as ex:\n    #         print(ex)\n\n    def is_downloaded(self):\n        \"\"\"Check whether a text archive has already been downloaded.\"\"\"\n        file = self.text_archive_dir / self.zip_file\n        if not os.path.exists(file):\n            return False\n        return os.path.getsize(file) != 0\n\n    def download_zip(self):\n        \"\"\"Download this Item's full text zip archive from cloud storage.\"\"\"\n        sas_token = os.getenv(self.SAS_ENV_VARIABLE).strip('\"')\n        if sas_token is None:\n            raise KeyError(\n                f\"The environment variable {self.SAS_ENV_VARIABLE} was not found.\"\n            )\n\n        url = self.FULLTEXT_STORAGE_ACCOUNT_URL\n        container = self.text_container\n        blob_name = str(Path(self.FULLTEXT_CONTAINER_PATH) / self.zip_file)\n        download_file_path = self.text_archive_dir / self.zip_file\n\n        # Make sure the archive download directory exists.\n        self.text_archive_dir.mkdir(parents=True, exist_ok=True)\n\n        if not os.path.exists(self.text_archive_dir):\n            raise RuntimeError(\n                f\"Failed to make archive download directory at {self.text_archive_dir}\"\n            )\n\n        # Download the blob archive.\n        try:\n            client = BlobClient(\n                url, container, blob_name=blob_name, credential=sas_token\n            )\n\n            with open(download_file_path, \"wb\") as download_file:\n                download_file.write(client.download_blob().readall())\n\n        except Exception as ex:\n            if \"status_code\" in str(ex):\n                print(\"Zip archive download failed.\")\n                print(\n                    f\"Ensure the {self.SAS_ENV_VARIABLE} env variable contains a valid SAS token\"\n                )\n\n            if os.path.exists(download_file_path):\n                if os.path.getsize(download_file_path) == 0:\n                    os.remove(download_file_path)\n                    print(f\"Removing empty download: {download_file_path}\")\n\n    def extract_fulltext_file(self):\n        \"\"\"Extract Item's full text file from a zip archive to DOWNLOAD_DIR.\"\"\"\n        archive = self.text_archive_dir / self.zip_file\n        with ZipFile(archive, \"r\") as zip_ref:\n            zip_ref.extract(str(self.text_path), path=self.text_extracted_dir)\n\n    def read_fulltext_file(self) -&gt; list[str]:\n        \"\"\"Read the full text for this Item from a file.\"\"\"\n        with open(self.text_extracted_dir / self.text_path) as f:\n            lines = f.readlines()\n        return lines\n\n    def extract_fulltext(self) -&gt; list[str]:\n        \"\"\"Extract the full text of this newspaper item.\"\"\"\n        # If the item full text has already been extracted, read it.\n        if os.path.exists(self.text_extracted_dir / self.text_path):\n            return self.read_fulltext_file()\n\n        if self.FULLTEXT_METHOD == \"download\":\n            # If not already available locally, download the full text archive.\n            if not self.is_downloaded():\n                self.download_zip()\n\n            if not self.is_downloaded():\n                raise RuntimeError(\n                    f\"Failed to download full text archive for item {self.item_code}: Expected finished download.\"\n                )\n\n            # Extract the text for this item.\n            self.extract_fulltext_file()\n\n        elif self.FULLTEXT_METHOD == \"blobfuse\":\n            raise NotImplementedError(\"Blobfuse access is not yet implemented.\")\n            blobfuse = \"/mounted/blob/storage/path/\"\n            zip_path = blobfuse / self.zip_file\n\n        else:\n            raise RuntimeError(\n                \"A valid fulltext access method must be selected: options are 'download' or 'blobfuse'.\"\n            )\n\n        # If the item full text still hasn't been extracted, report failure.\n        if not os.path.exists(self.text_extracted_dir / self.text_path):\n            raise RuntimeError(\n                f\"Failed to extract fulltext for {self.item_code}; path does not exist: {self.text_extracted_dir / self.text_path}\"\n            )\n\n        return self.read_fulltext_file()\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Item.download_dir","title":"download_dir  <code>property</code>","text":"<pre><code>download_dir\n</code></pre> <p>Path to the download directory for full text data.</p> <p>The DOWNLOAD_DIR class attribute contains the directory under which full text data will be stored. Users can change it by typing: Item.DOWNLOAD_DIR = \u201c/path/to/wherever/\u201d</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.text_archive_dir","title":"text_archive_dir  <code>property</code>","text":"<pre><code>text_archive_dir\n</code></pre> <p>Path to the storage directory for full text archives.</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.text_container","title":"text_container  <code>property</code>","text":"<pre><code>text_container\n</code></pre> <p>Azure blob storage container containing the Item full text.</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.text_extracted_dir","title":"text_extracted_dir  <code>property</code>","text":"<pre><code>text_extracted_dir\n</code></pre> <p>Path to the storage directory for extracted full text files.</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.text_path","title":"text_path  <code>property</code>","text":"<pre><code>text_path\n</code></pre> <p>Return a path relative to the full text file for this Item.</p> <p>This is generated from the zip archive (once downloaded and extracted) from the DOWNLOAD_DIR and the filename.</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.zip_file","title":"zip_file  <code>property</code>","text":"<pre><code>zip_file\n</code></pre> <p>Filename for this Item\u2019s zip archive containing the full text.</p>"},{"location":"reference/newspapers/models/#newspapers.models.Item.download_zip","title":"download_zip","text":"<pre><code>download_zip()\n</code></pre> <p>Download this Item\u2019s full text zip archive from cloud storage.</p> Source code in <code>newspapers/models.py</code> <pre><code>def download_zip(self):\n    \"\"\"Download this Item's full text zip archive from cloud storage.\"\"\"\n    sas_token = os.getenv(self.SAS_ENV_VARIABLE).strip('\"')\n    if sas_token is None:\n        raise KeyError(\n            f\"The environment variable {self.SAS_ENV_VARIABLE} was not found.\"\n        )\n\n    url = self.FULLTEXT_STORAGE_ACCOUNT_URL\n    container = self.text_container\n    blob_name = str(Path(self.FULLTEXT_CONTAINER_PATH) / self.zip_file)\n    download_file_path = self.text_archive_dir / self.zip_file\n\n    # Make sure the archive download directory exists.\n    self.text_archive_dir.mkdir(parents=True, exist_ok=True)\n\n    if not os.path.exists(self.text_archive_dir):\n        raise RuntimeError(\n            f\"Failed to make archive download directory at {self.text_archive_dir}\"\n        )\n\n    # Download the blob archive.\n    try:\n        client = BlobClient(\n            url, container, blob_name=blob_name, credential=sas_token\n        )\n\n        with open(download_file_path, \"wb\") as download_file:\n            download_file.write(client.download_blob().readall())\n\n    except Exception as ex:\n        if \"status_code\" in str(ex):\n            print(\"Zip archive download failed.\")\n            print(\n                f\"Ensure the {self.SAS_ENV_VARIABLE} env variable contains a valid SAS token\"\n            )\n\n        if os.path.exists(download_file_path):\n            if os.path.getsize(download_file_path) == 0:\n                os.remove(download_file_path)\n                print(f\"Removing empty download: {download_file_path}\")\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Item.extract_fulltext","title":"extract_fulltext","text":"<pre><code>extract_fulltext() -&gt; list[str]\n</code></pre> <p>Extract the full text of this newspaper item.</p> Source code in <code>newspapers/models.py</code> <pre><code>def extract_fulltext(self) -&gt; list[str]:\n    \"\"\"Extract the full text of this newspaper item.\"\"\"\n    # If the item full text has already been extracted, read it.\n    if os.path.exists(self.text_extracted_dir / self.text_path):\n        return self.read_fulltext_file()\n\n    if self.FULLTEXT_METHOD == \"download\":\n        # If not already available locally, download the full text archive.\n        if not self.is_downloaded():\n            self.download_zip()\n\n        if not self.is_downloaded():\n            raise RuntimeError(\n                f\"Failed to download full text archive for item {self.item_code}: Expected finished download.\"\n            )\n\n        # Extract the text for this item.\n        self.extract_fulltext_file()\n\n    elif self.FULLTEXT_METHOD == \"blobfuse\":\n        raise NotImplementedError(\"Blobfuse access is not yet implemented.\")\n        blobfuse = \"/mounted/blob/storage/path/\"\n        zip_path = blobfuse / self.zip_file\n\n    else:\n        raise RuntimeError(\n            \"A valid fulltext access method must be selected: options are 'download' or 'blobfuse'.\"\n        )\n\n    # If the item full text still hasn't been extracted, report failure.\n    if not os.path.exists(self.text_extracted_dir / self.text_path):\n        raise RuntimeError(\n            f\"Failed to extract fulltext for {self.item_code}; path does not exist: {self.text_extracted_dir / self.text_path}\"\n        )\n\n    return self.read_fulltext_file()\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Item.extract_fulltext_file","title":"extract_fulltext_file","text":"<pre><code>extract_fulltext_file()\n</code></pre> <p>Extract Item\u2019s full text file from a zip archive to DOWNLOAD_DIR.</p> Source code in <code>newspapers/models.py</code> <pre><code>def extract_fulltext_file(self):\n    \"\"\"Extract Item's full text file from a zip archive to DOWNLOAD_DIR.\"\"\"\n    archive = self.text_archive_dir / self.zip_file\n    with ZipFile(archive, \"r\") as zip_ref:\n        zip_ref.extract(str(self.text_path), path=self.text_extracted_dir)\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Item.is_downloaded","title":"is_downloaded","text":"<pre><code>is_downloaded()\n</code></pre> <p>Check whether a text archive has already been downloaded.</p> Source code in <code>newspapers/models.py</code> <pre><code>def is_downloaded(self):\n    \"\"\"Check whether a text archive has already been downloaded.\"\"\"\n    file = self.text_archive_dir / self.zip_file\n    if not os.path.exists(file):\n        return False\n    return os.path.getsize(file) != 0\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Item.read_fulltext_file","title":"read_fulltext_file","text":"<pre><code>read_fulltext_file() -&gt; list[str]\n</code></pre> <p>Read the full text for this Item from a file.</p> Source code in <code>newspapers/models.py</code> <pre><code>def read_fulltext_file(self) -&gt; list[str]:\n    \"\"\"Read the full text for this Item from a file.\"\"\"\n    with open(self.text_extracted_dir / self.text_path) as f:\n        lines = f.readlines()\n    return lines\n</code></pre>"},{"location":"reference/newspapers/models/#newspapers.models.Newspaper","title":"Newspaper","text":"<p>             Bases: <code>NewspapersModel</code></p> <p>Newspaper, including title and place.</p> Source code in <code>newspapers/models.py</code> <pre><code>class Newspaper(NewspapersModel):\n    \"\"\"Newspaper, including title and place.\"\"\"\n\n    # TODO #55: publication_code should be unique? Currently unique (but not tested with BNA)\n    publication_code = models.CharField(max_length=600, default=None)\n    title = models.CharField(max_length=255, default=None)\n    location = models.CharField(max_length=255, default=None, blank=True, null=True)\n    place_of_publication = models.ForeignKey(\n        Place,\n        on_delete=models.SET_NULL,\n        null=True,\n        related_name=\"newspapers\",\n        related_query_name=\"newspaper\",\n    )\n\n    def __repr__(self):\n        return self.publication_code\n\n    def __str__(self):\n        return truncate_str(self.title, max_length=MAX_PRINT_SELF_STR_LENGTH)\n\n    class Meta:\n        indexes = [\n            models.Index(\n                fields=[\n                    \"publication_code\",\n                ]\n            )\n        ]\n</code></pre>"},{"location":"reference/newspapers/views/","title":"views","text":""},{"location":"reference/newspapers/management/","title":"management","text":""},{"location":"reference/newspapers/management/commands/","title":"commands","text":""},{"location":"reference/newspapers/management/commands/items/","title":"items","text":""},{"location":"reference/newspapers/management/commands/items/#newspapers.management.commands.items.Command","title":"Command","text":"<p>             Bases: <code>Command</code></p> Source code in <code>newspapers/management/commands/items.py</code> <pre><code>class Command(NewspapersFixture):\n    models = [Item]\n\n    def __init__(self, force=False):\n        self.force = force\n        super(NewspapersFixture, self).__init__()\n\n    def get_zipfiles(self, data_provider):\n        zipfiles = [x for x in Path(MOUNTPOINTS[data_provider]).glob(\"*.zip\")]\n        zipfiles.sort(key=lambda x: x.stat().st_size)\n        if REVERSE:\n            zipfiles.reverse()\n\n        return zipfiles\n\n    def get_cache_path(self, data_provider, newspaper_zip, add_nlp=None):\n        if data_provider == \"jisc\":\n            if not add_nlp:\n                self.test_parent(cache_path)\n                cache_path = Path(f\"./{item_cache}/{data_provider}\")\n                return cache_path\n\n            nlp = add_nlp\n        else:\n            nlp = newspaper_zip.name.split(\"_\")[0]\n\n        m = len([x for x in nlp[3:]]) - 2\n        valid_path_numbers = [x for x in nlp[3:]][:m]\n        cache_path = Path(\n            f\"./{item_cache}/{data_provider}/\" + \"/\".join(valid_path_numbers)\n        )\n        self.test_parent(cache_path)\n\n        return cache_path\n\n    @staticmethod\n    def test_parent(path):\n        if not path.exists():\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n    def build_cache(self):\n        \"\"\"Build a cache in a `jsonl` file structure of a list of `Items`.\n\n        Build a cache within a jsonl file which contains a list of\n        Items, in the following structure:\n\n        ./{item_cache}/{name of data provider}/2/2/0002246.jsonl\n        \"\"\"\n        for data_provider in DATA_PROVIDERS:\n            ZIPFILES = self.get_zipfiles(data_provider)\n\n            for newspaper_zip in (bar1 := tqdm(ZIPFILES, leave=False)):\n                bar1.set_description(f\"{data_provider} :: {newspaper_zip.name}\")\n\n                cache_path = self.get_cache_path(data_provider, newspaper_zip)\n\n                if data_provider != \"jisc\":\n                    nlp = newspaper_zip.name.split(\"_\")[0]\n                    cache_file = cache_path / f\"{nlp}.jsonl\"\n\n                    if cache_file.exists():\n                        continue\n\n                issue_xmls = [\n                    f.filename for f in zipfile.ZipFile(newspaper_zip).filelist\n                ]\n\n                with zipfile.ZipFile(newspaper_zip) as zf:\n                    for issue_file in (bar2 := tqdm(issue_xmls, leave=False)):\n                        bar2.set_description(f\"{Path(issue_file).parent}\")\n\n                        if data_provider == \"jisc\":\n                            nlp = None\n                            paper_abbr = newspaper_zip.name.split(\"_\")[0]\n\n                        with zf.open(issue_file) as inner:\n                            issue_xml = inner.read()\n\n                            if not issue_xml:\n                                continue\n\n                        root = ET.fromstring(issue_xml)\n\n                        if data_provider == \"jisc\":\n                            e = root.find(\"./publication\")\n                            nlp = e.attrib.get(\"id\")\n                            cache_path = self.get_cache_path(\n                                data_provider, newspaper_zip, nlp\n                            )\n                            cache_file = cache_path / f\"{nlp}.jsonl\"\n                            self.test_parent(cache_file)\n\n                            issue_identifier = nlp + \"\".join(issue_file.split(\"/\")[1:4])\n                        else:\n                            issue_identifier = \"\".join(issue_file.split(\"/\")[0:3])\n\n                        ingest = {\n                            f\"lwm_tool_{x.tag}\": x.text or \"\"\n                            for x in root.findall(\"./process/lwm_tool/*\")\n                        }\n\n                        digitisation = {\n                            x.tag: x.text or \"\"\n                            for x in root.findall(\"./process/*\")\n                            if x.tag\n                            in [\n                                \"xml_flavour\",\n                                \"software\",\n                                \"mets_namespace\",\n                                \"alto_namespace\",\n                            ]\n                        }\n                        e = root.find(\"./publication/issue/item\")\n\n                        item = {\n                            f\"{x.tag}\": x.text or \"\"\n                            for x in e.findall(\"*\")\n                            if x.tag\n                            in [\n                                \"title\",\n                                \"word_count\",\n                                \"ocr_quality_mean\",\n                                \"ocr_quality_sd\",\n                                \"plain_text_file\",\n                                \"item_type\",\n                            ]\n                        }\n                        item[\"item_code\"] = issue_identifier + \"-\" + e.attrib.get(\"id\")\n                        item[\"input_filename\"] = item.get(\"plain_text_file\", \"\")\n                        del item[\"plain_text_file\"]\n\n                        item[\"ocr_quality_mean\"] = item.get(\"ocr_quality_mean\", 0)\n                        item[\"ocr_quality_sd\"] = item.get(\"ocr_quality_sd\", 0)\n\n                        # relations\n                        item[\"digitisation__software\"] = digitisation.get(\n                            \"software\", \"\"\n                        )\n                        item[\"ingest__lwm_tool_name\"] = ingest.get(\"lwm_tool_name\", \"\")\n                        item[\"ingest__lwm_tool_version\"] = ingest.get(\n                            \"lwm_tool_version\", \"\"\n                        )\n                        item[\"issue__issue_identifier\"] = issue_identifier\n                        item[\"data_provider\"] = data_provider\n\n                        # ensure length is right\n                        # -&gt; title needs to follow JSON's max limit\n                        item[\"title\"] = item.get(\"title\", \"\")[:2097152]\n                        # -&gt; item_code needs to follow db limit (set in newspapers.models)\n                        item[\"item_code\"] = item.get(\"item_code\", \"\")[:600]\n\n                        with open(cache_file, \"a+\") as f:\n                            f.write(f\"{json.dumps(item)}\\n\")\n\n    def ingest_cache(self):\n        total = 0\n\n        for data_provider in DATA_PROVIDERS:\n            JSONL_FILES = list(\n                Path(f\"./{item_cache}/{data_provider}/\").glob(\"**/*.jsonl\")\n            )\n\n            for jsonl_path in (bar1 := tqdm(JSONL_FILES)):\n                bar1.set_description(f\"{data_provider} :: {jsonl_path.name}\")\n\n                lines = jsonl_path.read_text().splitlines()\n\n                for line in (bar2 := tqdm(lines, leave=False)):\n                    item = json.loads(line)\n\n                    if not item.get(\"item_code\"):\n                        self.stdout.write(\n                            self.style.WARNING(\n                                f\"Warning: skipping one item in {jsonl_path.name} because it has no (required) item_code assigned.\"\n                            )\n                        )\n                        continue\n\n                    bar2.set_description(f\"{total} saved :: {item['item_code']}\")\n\n                    # relations\n                    digitisation_o = Digitisation.objects.get(\n                        software=item.get(\"digitisation__software\")\n                    )\n                    ingest_o = Ingest.objects.get(\n                        lwm_tool_name=item.get(\"ingest__lwm_tool_name\"),\n                        lwm_tool_version=item.get(\"ingest__lwm_tool_version\"),\n                    )\n                    data_provider_o = DataProvider.objects.get(\n                        name=item.get(\"data_provider\")\n                    )\n                    issue_o = Issue.objects.get(\n                        issue_code=item.get(\"issue__issue_identifier\")\n                    )\n\n                    del item[\"digitisation__software\"]\n                    del item[\"ingest__lwm_tool_name\"]\n                    del item[\"ingest__lwm_tool_version\"]\n                    del item[\"data_provider\"]\n                    del item[\"issue__issue_identifier\"]\n\n                    item[\"digitisation\"] = digitisation_o\n                    item[\"ingest\"] = ingest_o\n                    item[\"data_provider\"] = data_provider_o\n                    item[\"issue\"] = issue_o\n\n                    if not item[\"ocr_quality_mean\"] or item[\"ocr_quality_mean\"] == \"\":\n                        item[\"ocr_quality_mean\"] = 0\n\n                    if not item[\"ocr_quality_sd\"] or item[\"ocr_quality_sd\"] == \"\":\n                        item[\"ocr_quality_sd\"] = 0\n\n                    if Item.objects.filter(**item):\n                        # improving speed\n                        continue\n\n                    # write to db\n                    try:\n                        item_o, _ = Item.objects.update_or_create(\n                            item_code=item[\"item_code\"], defaults=item\n                        )\n                        total += 1\n                        # self.stdout.write(\n                        #     self.style.SUCCESS(f\"Item {item_o.id} written to db\")\n                        # )\n                    except OperationalError as e:\n                        if \"database is locked\" in str(e):\n                            self.stdout.write(\n                                self.style.WARNING(\n                                    f\"Warning: database is locked. Cannot write Item.\"\n                                )\n                            )\n</code></pre>"},{"location":"reference/newspapers/management/commands/items/#newspapers.management.commands.items.Command.build_cache","title":"build_cache","text":"<pre><code>build_cache()\n</code></pre> <p>Build a cache in a <code>jsonl</code> file structure of a list of <code>Items</code>.</p> <p>Build a cache within a jsonl file which contains a list of Items, in the following structure:</p> <p>./{item_cache}/{name of data provider}/2/2/0002246.jsonl</p> Source code in <code>newspapers/management/commands/items.py</code> <pre><code>def build_cache(self):\n    \"\"\"Build a cache in a `jsonl` file structure of a list of `Items`.\n\n    Build a cache within a jsonl file which contains a list of\n    Items, in the following structure:\n\n    ./{item_cache}/{name of data provider}/2/2/0002246.jsonl\n    \"\"\"\n    for data_provider in DATA_PROVIDERS:\n        ZIPFILES = self.get_zipfiles(data_provider)\n\n        for newspaper_zip in (bar1 := tqdm(ZIPFILES, leave=False)):\n            bar1.set_description(f\"{data_provider} :: {newspaper_zip.name}\")\n\n            cache_path = self.get_cache_path(data_provider, newspaper_zip)\n\n            if data_provider != \"jisc\":\n                nlp = newspaper_zip.name.split(\"_\")[0]\n                cache_file = cache_path / f\"{nlp}.jsonl\"\n\n                if cache_file.exists():\n                    continue\n\n            issue_xmls = [\n                f.filename for f in zipfile.ZipFile(newspaper_zip).filelist\n            ]\n\n            with zipfile.ZipFile(newspaper_zip) as zf:\n                for issue_file in (bar2 := tqdm(issue_xmls, leave=False)):\n                    bar2.set_description(f\"{Path(issue_file).parent}\")\n\n                    if data_provider == \"jisc\":\n                        nlp = None\n                        paper_abbr = newspaper_zip.name.split(\"_\")[0]\n\n                    with zf.open(issue_file) as inner:\n                        issue_xml = inner.read()\n\n                        if not issue_xml:\n                            continue\n\n                    root = ET.fromstring(issue_xml)\n\n                    if data_provider == \"jisc\":\n                        e = root.find(\"./publication\")\n                        nlp = e.attrib.get(\"id\")\n                        cache_path = self.get_cache_path(\n                            data_provider, newspaper_zip, nlp\n                        )\n                        cache_file = cache_path / f\"{nlp}.jsonl\"\n                        self.test_parent(cache_file)\n\n                        issue_identifier = nlp + \"\".join(issue_file.split(\"/\")[1:4])\n                    else:\n                        issue_identifier = \"\".join(issue_file.split(\"/\")[0:3])\n\n                    ingest = {\n                        f\"lwm_tool_{x.tag}\": x.text or \"\"\n                        for x in root.findall(\"./process/lwm_tool/*\")\n                    }\n\n                    digitisation = {\n                        x.tag: x.text or \"\"\n                        for x in root.findall(\"./process/*\")\n                        if x.tag\n                        in [\n                            \"xml_flavour\",\n                            \"software\",\n                            \"mets_namespace\",\n                            \"alto_namespace\",\n                        ]\n                    }\n                    e = root.find(\"./publication/issue/item\")\n\n                    item = {\n                        f\"{x.tag}\": x.text or \"\"\n                        for x in e.findall(\"*\")\n                        if x.tag\n                        in [\n                            \"title\",\n                            \"word_count\",\n                            \"ocr_quality_mean\",\n                            \"ocr_quality_sd\",\n                            \"plain_text_file\",\n                            \"item_type\",\n                        ]\n                    }\n                    item[\"item_code\"] = issue_identifier + \"-\" + e.attrib.get(\"id\")\n                    item[\"input_filename\"] = item.get(\"plain_text_file\", \"\")\n                    del item[\"plain_text_file\"]\n\n                    item[\"ocr_quality_mean\"] = item.get(\"ocr_quality_mean\", 0)\n                    item[\"ocr_quality_sd\"] = item.get(\"ocr_quality_sd\", 0)\n\n                    # relations\n                    item[\"digitisation__software\"] = digitisation.get(\n                        \"software\", \"\"\n                    )\n                    item[\"ingest__lwm_tool_name\"] = ingest.get(\"lwm_tool_name\", \"\")\n                    item[\"ingest__lwm_tool_version\"] = ingest.get(\n                        \"lwm_tool_version\", \"\"\n                    )\n                    item[\"issue__issue_identifier\"] = issue_identifier\n                    item[\"data_provider\"] = data_provider\n\n                    # ensure length is right\n                    # -&gt; title needs to follow JSON's max limit\n                    item[\"title\"] = item.get(\"title\", \"\")[:2097152]\n                    # -&gt; item_code needs to follow db limit (set in newspapers.models)\n                    item[\"item_code\"] = item.get(\"item_code\", \"\")[:600]\n\n                    with open(cache_file, \"a+\") as f:\n                        f.write(f\"{json.dumps(item)}\\n\")\n</code></pre>"},{"location":"reference/newspapers/management/commands/newspapers/","title":"newspapers","text":""},{"location":"reference/newspapers/management/commands/newspapers/#newspapers.management.commands.newspapers.Command","title":"Command","text":"<p>             Bases: <code>Fixture</code></p> Source code in <code>newspapers/management/commands/newspapers.py</code> <pre><code>class Command(Fixture):\n    app_name = \"newspapers\"\n    models = [Newspaper, Issue, Digitisation, Ingest, DataProvider]\n\n    def __init__(self, force=False):\n        self.force = force\n        super(Fixture, self).__init__()\n\n    def save_fixtures(self):\n        for model in self.models:\n            filename = f\"{model._meta.label.split('.')[-1]}-fixtures.json\"\n            model_fields = model._meta.get_fields()\n            data = serialize(\n                \"json\",\n                model.objects.all(),\n                fields=[\n                    x.name\n                    for x in model_fields\n                    if not x.name in [\"created_at\", \"updated_at\"]\n                ],\n            )\n            path = self.get_output_dir() / filename\n            with open(path, \"w+\") as f:\n                f.write(data)\n\n        return True\n\n    def get_zipfiles(self, data_provider):\n        zipfiles = [x for x in Path(MOUNTPOINTS[data_provider]).glob(\"*.zip\")]\n        zipfiles.sort(key=lambda x: x.stat().st_size)\n\n        if REVERSE:\n            zipfiles.reverse()\n\n        return zipfiles\n\n    def get_cache_path(self, data_provider, newspaper_zip, add_nlp=None):\n        if data_provider == \"jisc\":\n            if not add_nlp:\n                cache_path = Path(f\"./{newspaper_cache}/{data_provider}\")\n                self.test_parent(cache_path)\n                return cache_path\n\n            nlp = add_nlp\n        else:\n            nlp = newspaper_zip.name.split(\"_\")[0]\n\n        m = len([x for x in nlp[3:]]) - 2\n        valid_path_numbers = [x for x in nlp[3:]][:m]\n        cache_path = Path(\n            f\"./{newspaper_cache}/{data_provider}/\" + \"/\".join(valid_path_numbers)\n        )\n        self.test_parent(cache_path)\n\n        return cache_path\n\n    @staticmethod\n    def test_parent(path):\n        if not path.exists():\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n    def build_cache(self):\n        \"\"\"Build a cache in a file structure of Newspapers and Issues.\n\n        Caches follow this file structure:\n        ./{newspaper_cache}/{name of data provider}/2/2/0002246.json\n\n        Each json file contains either an object (Newspapers) or a list\n        of issues (Issues).\n        \"\"\"\n        UNNAMED = 0\n\n        for data_provider in DATA_PROVIDERS:\n            ZIPFILES = self.get_zipfiles(data_provider)\n\n            for newspaper_zip in (bar1 := tqdm(ZIPFILES)):\n                bar1.set_description(f\"{data_provider} :: {newspaper_zip.name}\")\n                collected = array([])\n\n                cache_path = self.get_cache_path(data_provider, newspaper_zip)\n\n                issue_xmls = [\n                    f.filename for f in zipfile.ZipFile(newspaper_zip).filelist\n                ]\n\n                with zipfile.ZipFile(newspaper_zip) as zf:\n                    for issue_file in (bar2 := tqdm(issue_xmls, leave=False)):\n                        bar2.set_description(f\"{Path(issue_file).parent}\")\n                        if data_provider == \"jisc\":\n                            nlp = None\n                            paper_abbr = newspaper_zip.name.split(\"_\")[0]\n\n                        with zf.open(issue_file) as inner:\n                            issue_xml = inner.read()\n\n                            if not issue_xml:\n                                continue\n\n                        root = ET.fromstring(issue_xml)\n\n                        e = root.find(\"./publication\")\n                        newspaper = {\"publication_code\": e.attrib.get(\"id\")}\n\n                        if data_provider == \"jisc\":\n                            if nlp == None:\n                                nlp = newspaper[\"publication_code\"]\n                                cache_path = self.get_cache_path(\n                                    data_provider, newspaper_zip, nlp\n                                )\n\n                            issue_identifier = nlp + \"\".join(issue_file.split(\"/\")[1:4])\n                        else:\n                            issue_identifier = \"\".join(issue_file.split(\"/\")[0:3])\n\n                        newspaper_cache_file = cache_path / f\"newspaper/{nlp}.json\"\n                        self.test_parent(newspaper_cache_file)\n\n                        if not newspaper_cache_file.exists():\n                            _newspaper = {\n                                x.tag: x.text or \"\"\n                                for x in e.findall(\"*\")\n                                if x.tag in [\"title\", \"location\"]\n                            }\n\n                            if not _newspaper.get(\"title\"):\n                                if data_provider == \"jisc\":\n                                    newspaper[\"title\"] = f\"{paper_abbr}\"\n                                else:\n                                    UNNAMED += 1\n                                    newspaper[\"title\"] = f\"Untitled {UNNAMED}\"\n\n                            newspaper = dict(newspaper, **_newspaper)\n\n                            newspaper_cache_file.write_text(json.dumps(newspaper))\n\n                        issue_cache_path = cache_path / Path(\n                            f\"issue/{nlp}/issues.jsonl\"\n                        )\n                        self.test_parent(issue_cache_path)\n\n                        if not issue_identifier in collected:\n                            # Create/build `current_issues``\n                            try:\n                                current_issues = [\n                                    json.loads(line)\n                                    for line in issue_cache_path.read_text().splitlines()\n                                ]\n                            except FileNotFoundError:\n                                current_issues = []\n\n                            # Check if already processed (i.e. in `current_issues`)\n                            if any(\n                                [\n                                    issue.get(\"issue_code\") == issue_identifier\n                                    for issue in current_issues\n                                ]\n                            ):\n                                collected = append(collected, issue_identifier)\n                                continue\n\n                            e = root.find(\"./publication/issue\")\n                            issue = {\n                                \"issue_code\": issue_identifier,\n                                \"publication__publication_code\": newspaper[\n                                    \"publication_code\"\n                                ],\n                            }\n\n                            if data_provider == \"jisc\":\n                                pass\n\n                            _issue = {\n                                f\"issue_{x.tag}\": x.text or \"\"\n                                for x in e.findall(\"*\")\n                                if x.tag in [\"date\"]\n                            }\n                            _issue[\"input_sub_path\"] = root.find(\n                                \"./process/input_sub_path\"\n                            ).text\n\n                            issue = dict(issue, **_issue)\n\n                            current_issues.append(issue)\n\n                            current_issues = list(\n                                {json.dumps(issue) for issue in current_issues}\n                            )\n\n                            with open(\n                                (cache_path / Path(f\"issue/{nlp}/issues.jsonl\")), \"w+\"\n                            ) as f:\n                                f.write(\"\\n\".join(current_issues))\n                                collected = append(collected, issue_identifier)\n\n    def ingest_cache(self):\n        \"\"\"Function for ingesting cache files for Newspaper and Issue items.\"\"\"\n        get_newspaper_files = lambda data_provider: [\n            x for x in Path(f\"./{newspaper_cache}/{data_provider}/\").glob(\"**/*.json\")\n        ]\n\n        get_issue_files = lambda data_provider: [\n            x\n            for x in Path(f\"./{newspaper_cache}/{data_provider}/\").glob(\"**/*.jsonl\")\n            if x.name == \"issues.jsonl\"\n        ]\n\n        def error_msg(kind, **kwargs):\n            if kwargs.get(\"required\"):\n                self.stdout.write(\n                    self.style.WARNING(\n                        f\"Warning: Skipping {kind} in {kwargs.get('json_path')} because it has no (required) {kwargs.get('required')}.\"\n                    )\n                )\n            elif kwargs.get(\"locked\"):\n                self.stdout.write(\n                    self.style.WARNING(\n                        f\"Warning: database is locked. Cannot write {kind}.\"\n                    )\n                )\n\n        def success_msg(kind, id):\n            if WRITE_SUCCESS:\n                self.stdout.write(self.style.SUCCESS(f\"Wrote {kind} {id} to db.\"))\n\n        total = 0\n\n        for data_provider in DATA_PROVIDERS:\n            # Start processing newspapers\n            NEWSPAPER_FILES = get_newspaper_files(data_provider)\n\n            for json_path in (bar1 := tqdm(NEWSPAPER_FILES, leave=False)):\n                bar1.set_description(\n                    f\"newspaper cache :: {data_provider} :: {json_path.name}\"\n                )\n\n                newspaper = json.loads(json_path.read_text())\n\n                if not newspaper.get(\"publication_code\"):\n                    error_msg(\n                        \"newspaper\", required=\"publication_code\", json_path=json_path\n                    )\n                    continue\n\n                try:\n                    o, _ = Newspaper.objects.update_or_create(\n                        publication_code=newspaper[\"publication_code\"],\n                        defaults=newspaper,\n                    )\n                    success_msg(\"Newspaper\", o.id)\n\n                except OperationalError as e:\n                    if \"database is locked\" in str(e):\n                        error_msg(\"Newspaper\", locked=True)\n                        continue\n\n            # Then process issues\n            ISSUE_FILES = get_issue_files(data_provider)\n\n            for json_path in (bar1 := tqdm(ISSUE_FILES)):\n                bar1.set_description(\n                    f\"issue cache :: {data_provider} :: {json_path.parent.name}\"\n                )\n\n                issues = [\n                    json.loads(line) for line in json_path.read_text().splitlines()\n                ]\n\n                for issue in (bar2 := tqdm(issues, leave=False)):\n                    if not issue.get(\"issue_code\"):\n                        error_msg(\"issue\", required=\"issue_code\", json_path=json_path)\n                        continue\n\n                    bar2.set_description(f\"{total} saved :: {issue['issue_date']}\")\n\n                    if Issue.objects.filter(issue_code=issue[\"issue_code\"]).count():\n                        # there is already an issue - we might want to overwrite, but testing for speed here.\n                        continue\n\n                    if not issue.get(\"publication__publication_code\"):\n                        error_msg(\n                            \"issue\",\n                            required=\"publication_code\",\n                            json_path=json_path,\n                        )\n                        continue\n\n                    # connections\n                    try:\n                        issue[\"newspaper\"] = Newspaper.objects.get(\n                            publication_code=issue[\"publication__publication_code\"]\n                        )\n                    except Newspaper.DoesNotExist:\n                        error_msg(\n                            \"issue\",\n                            required=f\"related newspaper ({issue['publication__publication_code']}) in db\",\n                            json_path=json_path,\n                        )\n                        continue\n\n                    del issue[\"publication__publication_code\"]\n\n                    try:\n                        o, _ = Issue.objects.update_or_create(\n                            issue_code=issue[\"issue_code\"], defaults=issue\n                        )\n                        total += 1\n                        success_msg(\"Issue\", o.id)\n                    except OperationalError as e:\n                        if \"database is locked\" in str(e):\n                            error_msg(\"Newspaper\", locked=True)\n                            continue\n</code></pre>"},{"location":"reference/newspapers/management/commands/newspapers/#newspapers.management.commands.newspapers.Command.build_cache","title":"build_cache","text":"<pre><code>build_cache()\n</code></pre> <p>Build a cache in a file structure of Newspapers and Issues.</p> <p>Caches follow this file structure: ./{newspaper_cache}/{name of data provider}/2/2/0002246.json</p> <p>Each json file contains either an object (Newspapers) or a list of issues (Issues).</p> Source code in <code>newspapers/management/commands/newspapers.py</code> <pre><code>def build_cache(self):\n    \"\"\"Build a cache in a file structure of Newspapers and Issues.\n\n    Caches follow this file structure:\n    ./{newspaper_cache}/{name of data provider}/2/2/0002246.json\n\n    Each json file contains either an object (Newspapers) or a list\n    of issues (Issues).\n    \"\"\"\n    UNNAMED = 0\n\n    for data_provider in DATA_PROVIDERS:\n        ZIPFILES = self.get_zipfiles(data_provider)\n\n        for newspaper_zip in (bar1 := tqdm(ZIPFILES)):\n            bar1.set_description(f\"{data_provider} :: {newspaper_zip.name}\")\n            collected = array([])\n\n            cache_path = self.get_cache_path(data_provider, newspaper_zip)\n\n            issue_xmls = [\n                f.filename for f in zipfile.ZipFile(newspaper_zip).filelist\n            ]\n\n            with zipfile.ZipFile(newspaper_zip) as zf:\n                for issue_file in (bar2 := tqdm(issue_xmls, leave=False)):\n                    bar2.set_description(f\"{Path(issue_file).parent}\")\n                    if data_provider == \"jisc\":\n                        nlp = None\n                        paper_abbr = newspaper_zip.name.split(\"_\")[0]\n\n                    with zf.open(issue_file) as inner:\n                        issue_xml = inner.read()\n\n                        if not issue_xml:\n                            continue\n\n                    root = ET.fromstring(issue_xml)\n\n                    e = root.find(\"./publication\")\n                    newspaper = {\"publication_code\": e.attrib.get(\"id\")}\n\n                    if data_provider == \"jisc\":\n                        if nlp == None:\n                            nlp = newspaper[\"publication_code\"]\n                            cache_path = self.get_cache_path(\n                                data_provider, newspaper_zip, nlp\n                            )\n\n                        issue_identifier = nlp + \"\".join(issue_file.split(\"/\")[1:4])\n                    else:\n                        issue_identifier = \"\".join(issue_file.split(\"/\")[0:3])\n\n                    newspaper_cache_file = cache_path / f\"newspaper/{nlp}.json\"\n                    self.test_parent(newspaper_cache_file)\n\n                    if not newspaper_cache_file.exists():\n                        _newspaper = {\n                            x.tag: x.text or \"\"\n                            for x in e.findall(\"*\")\n                            if x.tag in [\"title\", \"location\"]\n                        }\n\n                        if not _newspaper.get(\"title\"):\n                            if data_provider == \"jisc\":\n                                newspaper[\"title\"] = f\"{paper_abbr}\"\n                            else:\n                                UNNAMED += 1\n                                newspaper[\"title\"] = f\"Untitled {UNNAMED}\"\n\n                        newspaper = dict(newspaper, **_newspaper)\n\n                        newspaper_cache_file.write_text(json.dumps(newspaper))\n\n                    issue_cache_path = cache_path / Path(\n                        f\"issue/{nlp}/issues.jsonl\"\n                    )\n                    self.test_parent(issue_cache_path)\n\n                    if not issue_identifier in collected:\n                        # Create/build `current_issues``\n                        try:\n                            current_issues = [\n                                json.loads(line)\n                                for line in issue_cache_path.read_text().splitlines()\n                            ]\n                        except FileNotFoundError:\n                            current_issues = []\n\n                        # Check if already processed (i.e. in `current_issues`)\n                        if any(\n                            [\n                                issue.get(\"issue_code\") == issue_identifier\n                                for issue in current_issues\n                            ]\n                        ):\n                            collected = append(collected, issue_identifier)\n                            continue\n\n                        e = root.find(\"./publication/issue\")\n                        issue = {\n                            \"issue_code\": issue_identifier,\n                            \"publication__publication_code\": newspaper[\n                                \"publication_code\"\n                            ],\n                        }\n\n                        if data_provider == \"jisc\":\n                            pass\n\n                        _issue = {\n                            f\"issue_{x.tag}\": x.text or \"\"\n                            for x in e.findall(\"*\")\n                            if x.tag in [\"date\"]\n                        }\n                        _issue[\"input_sub_path\"] = root.find(\n                            \"./process/input_sub_path\"\n                        ).text\n\n                        issue = dict(issue, **_issue)\n\n                        current_issues.append(issue)\n\n                        current_issues = list(\n                            {json.dumps(issue) for issue in current_issues}\n                        )\n\n                        with open(\n                            (cache_path / Path(f\"issue/{nlp}/issues.jsonl\")), \"w+\"\n                        ) as f:\n                            f.write(\"\\n\".join(current_issues))\n                            collected = append(collected, issue_identifier)\n</code></pre>"},{"location":"reference/newspapers/management/commands/newspapers/#newspapers.management.commands.newspapers.Command.ingest_cache","title":"ingest_cache","text":"<pre><code>ingest_cache()\n</code></pre> <p>Function for ingesting cache files for Newspaper and Issue items.</p> Source code in <code>newspapers/management/commands/newspapers.py</code> <pre><code>def ingest_cache(self):\n    \"\"\"Function for ingesting cache files for Newspaper and Issue items.\"\"\"\n    get_newspaper_files = lambda data_provider: [\n        x for x in Path(f\"./{newspaper_cache}/{data_provider}/\").glob(\"**/*.json\")\n    ]\n\n    get_issue_files = lambda data_provider: [\n        x\n        for x in Path(f\"./{newspaper_cache}/{data_provider}/\").glob(\"**/*.jsonl\")\n        if x.name == \"issues.jsonl\"\n    ]\n\n    def error_msg(kind, **kwargs):\n        if kwargs.get(\"required\"):\n            self.stdout.write(\n                self.style.WARNING(\n                    f\"Warning: Skipping {kind} in {kwargs.get('json_path')} because it has no (required) {kwargs.get('required')}.\"\n                )\n            )\n        elif kwargs.get(\"locked\"):\n            self.stdout.write(\n                self.style.WARNING(\n                    f\"Warning: database is locked. Cannot write {kind}.\"\n                )\n            )\n\n    def success_msg(kind, id):\n        if WRITE_SUCCESS:\n            self.stdout.write(self.style.SUCCESS(f\"Wrote {kind} {id} to db.\"))\n\n    total = 0\n\n    for data_provider in DATA_PROVIDERS:\n        # Start processing newspapers\n        NEWSPAPER_FILES = get_newspaper_files(data_provider)\n\n        for json_path in (bar1 := tqdm(NEWSPAPER_FILES, leave=False)):\n            bar1.set_description(\n                f\"newspaper cache :: {data_provider} :: {json_path.name}\"\n            )\n\n            newspaper = json.loads(json_path.read_text())\n\n            if not newspaper.get(\"publication_code\"):\n                error_msg(\n                    \"newspaper\", required=\"publication_code\", json_path=json_path\n                )\n                continue\n\n            try:\n                o, _ = Newspaper.objects.update_or_create(\n                    publication_code=newspaper[\"publication_code\"],\n                    defaults=newspaper,\n                )\n                success_msg(\"Newspaper\", o.id)\n\n            except OperationalError as e:\n                if \"database is locked\" in str(e):\n                    error_msg(\"Newspaper\", locked=True)\n                    continue\n\n        # Then process issues\n        ISSUE_FILES = get_issue_files(data_provider)\n\n        for json_path in (bar1 := tqdm(ISSUE_FILES)):\n            bar1.set_description(\n                f\"issue cache :: {data_provider} :: {json_path.parent.name}\"\n            )\n\n            issues = [\n                json.loads(line) for line in json_path.read_text().splitlines()\n            ]\n\n            for issue in (bar2 := tqdm(issues, leave=False)):\n                if not issue.get(\"issue_code\"):\n                    error_msg(\"issue\", required=\"issue_code\", json_path=json_path)\n                    continue\n\n                bar2.set_description(f\"{total} saved :: {issue['issue_date']}\")\n\n                if Issue.objects.filter(issue_code=issue[\"issue_code\"]).count():\n                    # there is already an issue - we might want to overwrite, but testing for speed here.\n                    continue\n\n                if not issue.get(\"publication__publication_code\"):\n                    error_msg(\n                        \"issue\",\n                        required=\"publication_code\",\n                        json_path=json_path,\n                    )\n                    continue\n\n                # connections\n                try:\n                    issue[\"newspaper\"] = Newspaper.objects.get(\n                        publication_code=issue[\"publication__publication_code\"]\n                    )\n                except Newspaper.DoesNotExist:\n                    error_msg(\n                        \"issue\",\n                        required=f\"related newspaper ({issue['publication__publication_code']}) in db\",\n                        json_path=json_path,\n                    )\n                    continue\n\n                del issue[\"publication__publication_code\"]\n\n                try:\n                    o, _ = Issue.objects.update_or_create(\n                        issue_code=issue[\"issue_code\"], defaults=issue\n                    )\n                    total += 1\n                    success_msg(\"Issue\", o.id)\n                except OperationalError as e:\n                    if \"database is locked\" in str(e):\n                        error_msg(\"Newspaper\", locked=True)\n                        continue\n</code></pre>"},{"location":"reference/newspapers/migrations/","title":"migrations","text":""}]}